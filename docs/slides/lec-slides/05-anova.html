<!DOCTYPE html>
<html>
  <head>
    <title>Analysis of Variance</title>
    <meta charset="utf-8">
    <meta name="author" content="Dr. Maria Tackett" />
    <link rel="stylesheet" href="sta210-slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Analysis of Variance
### Dr. Maria Tackett
### 01.30.19

---




class: regular

## Announcements

- HW 01 and Lab 02 due **today at 11:59p**

- Make sure all files are updated to the final version on GitHub

---

## R Packages


```r
library(tidyverse)
library(broom)
library(knitr)
```

---

## Beer Data


```r
beer &lt;- read_csv("data/beer.csv") %&gt;%
  filter(!is.na(Carbohydrates), !is.na(PercentAlcohol))
```

- We remove observations with missing data, so the dataset contains information about 159 domestic beers

    - `Brand`
    - `Brewery`
    - `PercentAlcohol`
    - `CaloriesPer12Oz`
    - `Carbohydrates` (in grams)
---


### `Carbohydrates` vs. `PercentAlcohol`


```r
model &lt;- lm(Carbohydrates ~ PercentAlcohol, data=beer)
kable(model %&gt;% tidy(conf.int=TRUE),format="html",digits=3)
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; conf.low &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; conf.high &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.642 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.311 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.015 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.046 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.052 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.231 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; PercentAlcohol &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.799 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.242 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.443 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.322 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.277 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

### Distribution of  `Carbohydrates`


```r
ggplot(data=beer,mapping=aes(x=Carbohydrates)) +
  geom_histogram() 
```

&lt;img src="05-anova_files/figure-html/unnamed-chunk-2-1.png" width="60%" style="display: block; margin: auto;" /&gt;

**Goal:** To understand the variation in `Carbohydrates` 
---

## ANOVA 

The	&lt;font class="vocab"&gt;Analysis of Variance (ANOVA)&lt;/font&gt; approach	to	regression is used to answer the following:  
  - how	much	of	the	variation	in	the	response variable	can	be	attributed	to	the	regression	model
  -  how	much of the	variation	in	the	response can be attributed to factors not included in the model.
&lt;br&gt;&lt;br&gt;

--

|  | Sum of Squares | DF | Mean Square | F-Stat| p-value |
|------------------|----------------|--------------------|-------------|-------------|--------------------|
| Regression (Model) | `\(\sum\limits_{i=1}^{n}(\hat{y}_i - \bar{y})^2\)` | 1 | `\(MSS/1\)` | `\(MMS/RMS\)` | `\(P(F &gt; \text{F-Stat})\)` |
| Residual | `\(\sum\limits_{i=1}^{n}(y_i - \hat{y}_i)^2\)` | `\(n-2\)` | `\(RSS\)` / `\((n-2)\)` |  |  |
| Total | `\(\sum\limits_{i=1}^{n}(y_i - \bar{y})^2\)` | `\(n-1\)` | `\(TSS\)` / `\((n-1)\)` |  |  |

---


## Total variation 

The total amount of variation in the `\(n\)` observed values of `\(y\)` is the &lt;font class="vocab"&gt;Total Sum of Squares&lt;/font&gt;. You can think of this as the variation about the sample mean, `\(\bar{y}\)`.

`$$TSS = \sum\limits_{i=1}^n (y_i - \bar{y})^2 = (n-1)s_y^2$$`

&lt;img src="05-anova_files/figure-html/unnamed-chunk-3-1.png" width="65%" style="display: block; margin: auto;" /&gt;

---

## Regression variation 

The variation of the predicted values about the overall mean is the  &lt;font class="vocab"&gt;Regression (Model) Sum of Squares&lt;/font&gt;. You can think of this as how much information about `\(y\)` is explained by its relationship with `\(x\)`.

`$$MSS = \sum\limits_{i=1}^n (\hat{y}_i - \bar{y})^2$$`

&lt;img src="05-anova_files/figure-html/unnamed-chunk-4-1.png" width="65%" style="display: block; margin: auto;" /&gt;

---

## Residual variation

The amount of variation in the responses for a given value of `\(x\)` is the &lt;font class="vocab"&gt;Residual Sum of Squares&lt;/font&gt;. You can think of this as the variation in the response values about the line if you know the value of `\(x\)`.

`$$RSS = \sum\limits_{i=1}^n (y_i- \hat{y}_i)^2$$`

&lt;img src="05-anova_files/figure-html/unnamed-chunk-5-1.png" width="65%" style="display: block; margin: auto;" /&gt;


---

## `\(R^2\)`

- We have already used the sum of squares in the ANOVA table to calculate `\(R^2\)`, the proportion of variation in the response that is explained by variation in the predictor variable, i.e. the regression line.
&lt;br&gt;&lt;br&gt;

.instructions[
`$$R^2 = \frac{\text{TSS} - \text{RSS}}{\text{TSS}} = 1 - \frac{\text{RSS}}{\text{TSS}}$$`
]
---

## Mean square

The &lt;font class="vocab"&gt;mean square (MS)&lt;/font&gt; is calculated by dividing a sum of squares by its associated degrees of freedom 
&lt;br&gt;&lt;br&gt;

`\(\text{Total MS} = \frac{\text{TSS}}{n-1} = s_y^2\)`
&lt;br&gt;&lt;br&gt;

`\(\text{Regression (Model) MS} = \frac{\text{MSS}}{1} = R^2(n-1)s_y^2\)`
&lt;br&gt;&lt;br&gt;

`\(\text{Residual MS} = \frac{\text{RSS}}{n-2} = \hat{\sigma}^2\)`


---

## Test for `\(\beta_1\)`

- We want to test if there is a significant linear relationship between the response and predictor variable

`$$\begin{aligned}&amp;H_0: \beta_1 = 0\\
&amp;H_a: \beta_1 \neq 0\\\end{aligned}$$`

- In the previous class, we tested these hypotheses using inference about the slope
 
- Today we will test the hypotheses using analysis of variance. 
    - This approach will be especially useful for multiple linear regression.
    
---

## Test statistic

- The &lt;font class="vocab3"&gt;test statistic &lt;/font&gt; for ANOVA is

`$$\text{F-statistic} = \frac{\text{Regression (Model) MS}}{\text{Residual MS}} = \frac{\text{MSS}/1}{{\text{RSS}}/(n-2)}$$`
&lt;br&gt;

- **Main idea:** If there is a significant linear relationship between `\(x\)` and `\(y\)`, the Regression MS will be much larger than the Residual MS. Therefore, a large F-statistic is evidence against `\(H_0\)`.

---

## p-value

- The test statistic under this model follows an `\(F\)` distribution with `\(1\)` and `\(n-2\)` degrees of freedom

- The `\(F\)` distribution is right-tailed, so to calculate the p-value, we use 

`$$\text{p-value} = P(F \geq \text{ F-statistic})$$`

&lt;img src="05-anova_files/figure-html/unnamed-chunk-6-1.png" width="70%" style="display: block; margin: auto;" /&gt;

---

## ANOVA Table

|  | Sum of Squares | DF | Mean Square | F-Stat| p-value |
|------------------|----------------|--------------------|-------------|-------------|--------------------|
| Regression (Model) | `\(\sum\limits_{i=1}^{n}(\hat{y}_i - \bar{y})^2\)` | 1 | `\(MSS/1\)` | `\(MMS/RMS\)` | `\(P(F &gt; \text{F-Stat})\)` |
| Residual | `\(\sum\limits_{i=1}^{n}(y_i - \hat{y}_i)^2\)` | `\(n-2\)` | `\(RSS\)` / `\((n-2)\)` |  |  |
| Total | `\(\sum\limits_{i=1}^{n}(y_i - \bar{y})^2\)` | `\(n-1\)` | `\(TSS\)` / `\((n-1)\)` |  |  |

---

### ANOVA for beer data


```r
kable(tidy(aov(model)),format="html",digits=3)
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; df &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; sumsq &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; meansq &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; PercentAlcohol &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1009.861 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1009.861 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 55.397 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Residuals &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 157 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2862.021 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 18.229 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; NA &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; NA &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;

.question[
- What are the total degrees of freedom? 

- What is `\(\hat{\sigma}^2\)`, the estimated regression variance? 

- Calculate and interpret `\(R^2\)`. 
]

---

class: middle, center

## Using ANOVA to compare group means

---

class: middle

Thus far, we have used a quantitative explanatory variable to understand the variation in a quantitative response variable.

Now, we will use a **categorical (qualtitatve) explanatory variable** to understand the variation in a quantitative response variable.

---

## Carbohydrates vs. Brewery

- Is there a relationship between the brewery and the total carbohydrates in beers from the top 5 domestic breweries? 



```r
top5_brewery &lt;- c("MillerCoors","Flying Dog Brewery", 
                  "Anheuser Busch",  "Sierra Nevada", "Budweiser")
beer &lt;- beer %&gt;%
  filter(Brewery %in% top5_brewery)
```

---

## Carbohydrates vs. Brewery


```r
ggplot(data=beer,mapping=aes(x=Brewery,y=Carbohydrates)) +
  geom_boxplot()
```

&lt;img src="05-anova_files/figure-html/unnamed-chunk-9-1.png" width="70%" style="display: block; margin: auto;" /&gt;

---

##Notation

- `\(K\)` is number of distinct groups. We index the groups as `\(k=1,\dots, K\)`.
&lt;br&gt;

--

- `\(n_k\)` is number of observations in group `\(k\)`
&lt;br&gt;

--

- `\(n=n_1 + n_2 + \dots + n_K\)` is the total number of observations
&lt;br&gt;

--
  
- `\(y_{kj}\)` is the `\(j^{th}\)` observation in group `\(k\)`, for all `\(k,j\)`
&lt;br&gt;

--

- `\(\mu_k\)` is the population mean for group `\(k\)`, for `\(k=1,\dots,K\)`

---

## Set Up

- Is there a significant relationship between the explanatory variable `\(x\)` and the response variable `\(y\)`? 

- &lt;font class="vocab"&gt;Goal: &lt;/font&gt; Test whether the group means are equal 
--

- ANOVA model assumes
`$$y_{kj} = \mu_{k} + \epsilon_{kj}$$`
such that `\(\epsilon_{kj}\)` is the amount `\(y_{kj}\)` deviates from `\(\mu_k\)`
--

- &lt;font class="vocab"&gt;Assumption: &lt;/font&gt; `\(\epsilon_{kj}\)` follows a Normal distribution with mean 0 and constant variance `\(\sigma^2\)`, i.e. `\(\epsilon_{kj} \sim N(0,\sigma^2)\)`
  + Another viewpoint: `\(y_{kj} \sim N(\mu_k, \sigma^2)\)`


---

## Hypotheses

- &lt;font class="vocab"&gt;General Question: &lt;/font&gt; Is there a significant difference in the means across the `\(K\)` groups?
--

- In other words, we want to test the following hypotheses: 

$$
`\begin{aligned}
&amp;H_0: \mu_1 = \mu_2 = \dots =  \mu_K\\
&amp;H_a: \text{At least one }\mu_i \text{ is not equal to the others}
\end{aligned}`
$$
--


- If the sample means are "far apart", " there is evidence against `\(H_0\)`
--


- We will calculate a test statistic to assess "far apart" in the context of the data
 
---

## ANOVA

- **Main Idea: ** Decompose the &lt;font color="green"&gt;total variation&lt;/font&gt; in the data into the variation &lt;font color="blue"&gt;between groups&lt;/font&gt; and the variation &lt;font color="red"&gt;within each group&lt;/font&gt;

`$$\color{green}{\sum_{k=1}^{K}\sum_{j=1}^{n_k}(y_{kj}- \bar{y})^2}=\color{blue}{\sum_{k=1}^{K}n_k(\bar{y}_k-\bar{y})^2} + \color{red}{\sum_{k=1}^{K}\sum_{j=1}^{n_k}(y_{kj}-\bar{y}_k)^2}$$`
&lt;br&gt;

--

- If the variation &lt;font color="blue"&gt;between groups&lt;/font&gt; is significantly greater than the variation &lt;font color="red"&gt;within each group&lt;/font&gt;, then there is evidence against the null hypothesis.

---

## ANOVA table for comparing means

.small[
|  | Sum of Squares | DF | Mean Square | F-Stat| p-value |
|------------------|----------------|--------------------|-------------|-------------|--------------------|
| Between (Model) | `\(\sum\limits_{k=1}^{K}n_k(\bar{y}_k-\bar{y})^2\)` | `\(K-1\)` | `\(SSB/(K-1)\)` | `\(MSB/MSW\)` | `\(P(F &gt; \text{F-Stat})\)` |
| Within (Residual) | `\(\sum\limits_{k=1}^{K}\sum\limits_{j=1}^{n_k}(y_{kj}-\bar{y}_k)^2\)` | `\(n-K\)` | `\(SSW/(n-K)\)` |  |  |
| Total | `\(\sum\limits_{k=1}^{K}\sum\limits_{j=1}^{n_k}(y_{kj}-\bar{y})^2\)` | `\(n-1\)` | `\(SST/(n-1)\)` |  |  |
]

---

## Total

- Variation between and within groups 

`$$SST =\sum_{k=1}^{K}\sum_{j=1}^{n_k}(y_{kj}-\bar{y})^2$$`
--

- &lt;font class="vocab"&gt;Degrees of freedom&lt;/font&gt; 
`$$DFT = n-1$$`
--

- Estimate of the variance across all observations: 
`$$\frac{SST}{DFT} = \frac{\sum_{k=1}^{K}\sum_{j=1}^{n_k}(y_{kj}-\bar{y})^2}{n-1}$$`

---

## Between (Model)

- Variation in the group means

`$$SSB = \sum_{k=1}^{K}n_k(\bar{y}_k-\bar{y})^2$$`
--

- &lt;font class="vocab"&gt;Degrees of freedom&lt;/font&gt; 
`$$DFB = K-1$$`
--

- &lt;font class="vocab"&gt;Mean Squares Between&lt;/font&gt;
`$$MSB = \frac{SSB}{DFB} = \frac{\sum_{k=1}^{K}n_i(\bar{y}_k-\bar{y})^2}{K-1}$$`
--

- MSB Estimates the variance across the `\(\mu_i\)`'s
  
  
---

class: regular 

## Within (Residual)

- Variation within each group

`$$SSW=\sum_{k=1}^{K}\sum_{j=1}^{n_k}(y_{kj}-\bar{y}_k)^2$$`
--

- &lt;font class="vocab"&gt;Degrees of freedom&lt;/font&gt; 
`$$DFW = n-K$$`
--

- &lt;font class="vocab"&gt;Mean Squares Within&lt;/font&gt;
`$$MSW = \frac{SSW}{DFW} = \frac{\sum_{k=1}^{K}\sum_{j=1}^{n_k}(y_{kj}-\bar{y}_i)^2}{n-K}$$`
--

- MSW is the estimate of `\(\sigma^2\)`, the variance within each group

---

## Carbohydrates vs. Brewery 


```r
model_brewery &lt;- lm(Carbohydrates ~ Brewery,data=beer)
kable(tidy(aov(model_brewery)),format="html",digits=3)
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; df &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; sumsq &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; meansq &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Brewery &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 962.055 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 240.514 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 14.02 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Residuals &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 88 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1509.696 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 17.156 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; NA &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; NA &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

.question[
- How many observations in the dataset are from the top 5 breweries?

- What is `\(\hat{\sigma}^2\)`, the estimated variance within each group? 

- State the null and alternative hypothesis for this test. What is your conclusion?
]


---

## Assumptions for ANOVA

- &lt;font class="vocab"&gt;Normality: &lt;/font&gt; `\(y_{kj} \sim N(\mu_k, \sigma^2)\)`

- &lt;font class="vocab"&gt;Independence: &lt;/font&gt; There is independence within and across groups

- &lt;font class="vocab"&gt;Constant Variance: &lt;/font&gt; The population distribution for each group has a common variance, `\(\sigma^2\)`

- We can typically check these assumptions in the exploratory data analysis
---

## Before next class

- [Reading 03](https://www2.stat.duke.edu/courses/Spring19/sta210.001/reading/reading-03.html) due Monday

- Understand inference and ANOVA for simple linear regression!
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
