<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Multiple Linear Regression</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr. Maria Tackett" />
    <link rel="stylesheet" href="sta210-slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Multiple Linear Regression
### Dr. Maria Tackett
### 02.05.20

---




class: middle, center

### [Click for PDF of slides](06-mlr.pdf)

---

### Announcements

- [HW 02](https://www2.stat.duke.edu/courses/Spring20/sta210.001/hw/hw-02.html) **due Wed, Feb 12 at 11:59p**

- [Reading for today](https://www2.stat.duke.edu/courses/Spring20/sta210.001/reading/reading-04.html)

- **READING FOR NEXT MONDAY**

---

### Today's Agenda 

- ANOVA

- Introducing multiple linear regression 



---

### ANOVA



.pull-left[
&lt;img src="06-mlr_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[

```
## # A tibble: 4 x 4
##   season     n  mean    sd
##   &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 Fall      25 5180. 1848.
## 2 Spring    23 4924. 1889.
## 3 Summer    27 5739. 1662.
## 4 Winter    25 2779. 1465.
```
]

---

### ANOVA for Capital Bike Share


`$$\begin{align}&amp;H_0: \mu_W = \mu_{Sp} = \mu_{Su} = \mu_{F} \\
&amp;H_a: \text{at least 1 }\mu_i \text{ is not equal to the others} \\\end{align}$$`
&lt;br&gt;

--


|          | Df|    Sum Sq|  Mean Sq| F value| Pr(&gt;F)|
|:---------|--:|---------:|--------:|-------:|------:|
|season    |  3| 128202929| 42734310|  14.458|      0|
|Residuals | 96| 283747246|  2955700|      NA|     NA|

---

### Calculate p-value 

- Calculate the p-value using an F distribution with `\(K-1\)` and `\(n-K\)` degrees of freedom. 

- In the Capital Bike Share example, the p-value is calculated from the F distribution with 3 and 96 degrees of freedom. 

&lt;img src="06-mlr_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;

**ADD LINK TO NOTES ABOUT F TEST STATISTIC**
---

### Assumptions for ANOVA

- &lt;font class="vocab"&gt;Normality: &lt;/font&gt; `\(y_{ij} \sim N(\mu_i, \sigma^2)\)`

- &lt;font class="vocab"&gt;Constant variance: &lt;/font&gt; The population distribution for each group has a common variance, `\(\sigma^2\)`

- &lt;font class="vocab"&gt;Independence: &lt;/font&gt; The observations are independent from one another
    - This applies to observation within and between groups

- We can typically check these assumptions in the exploratory data analysis

---

### Robustness to Assumptions

- &lt;font class="vocab"&gt;Normality: &lt;/font&gt; `\(y_{ij} \sim N(\mu_i, \sigma^2)\)`
  + ANOVA relatively robust to departures from Normality. 
  + Concern when there are strongly skewed distributions with different sample sizes (especially if sample sizes are small, &lt; 10 in each group)
  

- &lt;font class="vocab"&gt;Independence: &lt;/font&gt; There is independence within and across groups
  + If this doesn't hold, should use methods that account for correlated errors

---

### Robustness to Assumptions

- &lt;font class="vocab"&gt;Constant variance: &lt;/font&gt; The population distribution for each group has a common variance, `\(\sigma^2\)`
  + Critical assumption, since the pooled (combined) variance is important for ANOVA
  + **General rule:** If the sample sizes within each group are approximately equal, the results of the F-test are valid if the largest variance is no more than 4 times the smallest variance (i.e. the largest standard deviation is no more than 2 times the smallest standard deviation)
  
---

### Capital Bike Share: Normality


```r
ggplot(data = bikeshare, aes(x = count)) +
  geom_histogram() + 
  facet_wrap(~season) + 
  labs(title = "Daily bike rentals by season")
```

&lt;img src="06-mlr_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;

---

### Capital Bike Share: Constant Variance


```r
bikeshare %&gt;%
  group_by(season) %&gt;%
  summarise(sd = sd(count))
```

```
## # A tibble: 4 x 2
##   season    sd
##   &lt;chr&gt;  &lt;dbl&gt;
## 1 Fall   1848.
## 2 Spring 1889.
## 3 Summer 1662.
## 4 Winter 1465.
```

---

### Capital Bike Share: Independence

- Recall that the data is 100 randomly selected days in 2011 and 2012. 
- Let's look at the counts in date order to see if a pattern still exists

&lt;img src="06-mlr_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

--

Though the days were randomly selected, it still appears the independence assumption is violated. 
- Additional methods may be required to fully examine this data.

---

### Why not just use the model output?


|term         |  estimate| std.error| statistic| p.value|  conf.low| conf.high|
|:------------|---------:|---------:|---------:|-------:|---------:|---------:|
|(Intercept)  |  5180.200|   343.843|    15.066|   0.000|  4497.677|  5862.723|
|seasonSpring |  -256.591|   496.726|    -0.517|   0.607| -1242.585|   729.402|
|seasonSummer |   558.911|   477.178|     1.171|   0.244|  -388.279|  1506.101|
|seasonWinter | -2400.760|   486.267|    -4.937|   0.000| -3365.993| -1435.527|


- The model coefficients and associated hypothesis test / confidence interval are interpreted in relation to the baseline level
    - The coefficients, test statistics, confidence intervals, and p-values all change if the baseline category changes (more on this later!)

- An ANOVA test gives indication if **&lt;u&gt;any&lt;/u&gt;** category has a significantly different mean regardless of the baseline
    - The sum of squares, mean squares, test statistic, and p-value stay the same even if the baseline changes

---

class: middle, center

## Multiple Linear Regression

---
### Example: House prices - UPDATE DESCRIPTION!! 


- **Question**: 


---

 

### Data




```r
glimpse(homes)
```

```
## Observations: 85
## Variables: 7
## $ bedrooms     &lt;dbl&gt; 4, 4, 4, 5, 5, 4, 4, 4, 4, 3, 4, 4, 3, 4, 3, 5, 4, …
## $ bathrooms    &lt;dbl&gt; 1.0, 2.0, 2.0, 2.0, 2.5, 2.0, 1.0, 1.0, 1.5, 2.0, 2…
## $ living_area  &lt;dbl&gt; 1380, 1761, 1564, 2904, 1942, 1830, 1585, 941, 1481…
## $ lot_size     &lt;dbl&gt; 6000, 7400, 6000, 9898, 7788, 6000, 6000, 6800, 600…
## $ year_built   &lt;dbl&gt; 1948, 1951, 1948, 1949, 1948, 1948, 1948, 1951, 194…
## $ property_tax &lt;dbl&gt; 8360, 5754, 8982, 11664, 8120, 8197, 6223, 2448, 90…
## $ sale_price   &lt;dbl&gt; 350000, 360000, 350000, 375000, 370000, 335000, 295…
```

---

### Variables - UPDATE 

**Explanatory**
- &lt;font class="vocab"&gt;`Educ`: &lt;/font&gt;years of education
- &lt;font class="vocab"&gt;`Exper`: &lt;/font&gt;months of previous work experience (before hire at bank)
- &lt;font class="vocab"&gt;`Female`: &lt;/font&gt;1 if female, 0 if male
- &lt;font class="vocab"&gt;`Senior`: &lt;/font&gt;months worked at bank since hire
- &lt;font class="vocab"&gt;`Age`: &lt;/font&gt;age in months

**Response**
- &lt;font class="vocab"&gt;`Bsal`: &lt;/font&gt;annual salary at time of hire

---

### Exploratory data analysis

&lt;img src="06-mlr_files/figure-html/unnamed-chunk-13-1.png" style="display: block; margin: auto;" /&gt;


---

### Exploratory data analysis 

&lt;img src="06-mlr_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;" /&gt;

---

### Exploratory data anlaysis

&lt;img src="06-mlr_files/figure-html/unnamed-chunk-15-1.png" style="display: block; margin: auto;" /&gt;

---

class: middle

.question[
What is a disadvantage to fitting a separate model for each predictor variable? 
]

---

## Multiple Regression Model

We will calculate a multiple linear regression model with the following form: 
.alert[
.small[
`$$\begin{aligned}\text{sale_price} ~ = &amp; ~
\beta_0 + \beta_1 \text{bedrooms} + \beta_2 \text{bathrooms} + \beta_3 \text{living_area} \\
&amp;+ \beta_4 \text{lot_size} + \beta_5 \text{year_built} + \beta_6 \text{property_tax}\end{aligned}$$`
]
]

- Similar to simple linear regression, this model assumes that at each combination of the predictor variables, the values `sale_price` follow a Normal distribution

---

### Regression Model

- Recall: The simple linear regression model assumes 

`$$y|x\sim N(\beta_0 + \beta_1 x, \sigma^2)$$`
--

- Similarly: The multiple linear regression model assumes 

`$$y|x_1, x_2, \ldots, x_p \sim N(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p, \sigma^2)$$`
--

&lt;br&gt;


.alert[
For a given observation `\((x_{i1}, x_{i2} \ldots, x_{iP}, y_i)\)`

`$$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} + \epsilon_{i} \hspace{5mm} \epsilon_i \sim N(0,\sigma^2)$$`
]

---

### Regression Model 

- At any combination of `\(x's\)`, the true mean value of `\(y\)` is
`$$\mu_y = \beta_0 + \beta_1 x_{1} + \beta_2 x_2 + \dots + \beta_p x_p$$`
--

- We will use multiple linear regression to estimate the mean `\(y\)` for any combination of `\(x's\)`

`$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_{1} + \hat{\beta}_2 x_2 + \dots + \hat{\beta}_p x_{p}$$`

---

### Regression Output

.small[

```r
price_model &lt;- lm(sale_price ~ bedrooms + bathrooms + living_area + lot_size + year_built + property_tax, 
                  data = homes)

tidy(price_model, conf.int = TRUE) %&gt;%
  kable(format = "markdown", digits = 3)
```



|term         |     estimate|   std.error| statistic| p.value|      conf.low|  conf.high|
|:------------|------------:|-----------:|---------:|-------:|-------------:|----------:|
|(Intercept)  | -7148818.957| 3820093.694|    -1.871|   0.065| -14754041.291| 456403.376|
|bedrooms     |   -12291.011|    9346.727|    -1.315|   0.192|    -30898.915|   6316.893|
|bathrooms    |    51699.236|   13094.170|     3.948|   0.000|     25630.746|  77767.726|
|living_area  |       65.903|      15.979|     4.124|   0.000|        34.091|     97.715|
|lot_size     |       -0.897|       4.194|    -0.214|   0.831|        -9.247|      7.453|
|year_built   |     3760.898|    1962.504|     1.916|   0.059|      -146.148|   7667.944|
|property_tax |        1.476|       2.832|     0.521|   0.604|        -4.163|      7.115|
]

---

### Interpreting `\(\hat{\beta}_j\)`

- An estimated coefficient `\(\hat{\beta}_j\)` is the expected change in `\(y\)` to change when `\(x_j\)` increases by one unit **&lt;u&gt;holding the values of all other predictor variables constant&lt;/u&gt;**.

--

&lt;br&gt;

- *Example:* The estimated coefficient for **`living_area`** is 65.90. This means for each additional square foot of living area, we expect the sale price of a house in Levittown, NY to increase by $65.90, on average, holding all other predictor variables constant.

---

### Hypothesis Tests for `\(\hat{\beta}_j\)`

- We want to test whether a particular coefficient has a value of 0 in the population, given all other variables in the model: 

.alert[
`$$\begin{aligned}&amp;H_0: \beta_j = 0 \\ &amp;H_a: \beta_j \neq 0\end{aligned}$$`
]

- The test statistic reported in R is the following: 

`$$\text{test statistic} = t =  \frac{\hat{\beta}_j - 0}{SE(\hat{\beta}_j)}$$`

- Calculate the p-value using the `\(t\)` distribution with `\(n - p - 1\)` degrees of freedom, where `\(p\)` is the number of terms in the model (not including the intercept).

---

### UPDATE - In Class Activity 

.question[
Given the other variables in the model, are the following significant predictors of salary at time of hire (`Bsal`)? 

- Education (`Educ`)
- Experience (`Exper`)
]

---

### Confidence Interval for `\(\beta_j\)`

.alert[
The `\(C%\)` confidence interval for `\(\beta_j\)` 
`$$\hat{\beta}_j \pm t^* SE(\hat{\beta}_j)$$`
where `\(t^*\)` follows a `\(t\)` distribution with with `\((n - p - 1)\)` degrees of freedom
]

- **General Interpretation**: We are `\(C%\)` confident that the interval LB to UB contains the population coefficient of `\(x_j\)`. Therefore, for every one unit increase in `\(x_j\)`, we expect `\(y\)` to change by LB to UB units, holding all else constant. 

---

### CI for `bedrooms`


.question[
Interpret the 95% confidence interval for the coefficient of `Educ`.
]


---

### Notes about CI and Hypothesis Tests

- If the sample size is large enough, the test will likely result in rejecting `\(H_0: \beta_j=0\)` even `\(x_j\)` has a very small effect on `\(y\)`
  + Consider the &lt;font class="vocab"&gt;practical significance&lt;/font&gt; of the result not just the statistical significance 
  + Use the confidence interval to draw conclusions instead of p-values
 
--

- If the sample size is small, there may not be enough evidence to reject `\(H_0: \beta_j=0\)`
    - When you fail to reject the null hypothesis, **DON'T** immediately conclude that the variable has no association with the response. 
    - There may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association.
    

---

### Prediction

- We calculate predictions the same as with simple linear regression 

- **Example:** What is the predicted sale price for a house in Levittown, NY with 3 bedrooms, 1 bathroom, 1050 square feet of living area, 6000 square foot lot size, built in 1948 with $6306 in property taxes?



```r
-7148818.957 - 12291.011 * 3 + 51699.236 * 1 + 
  65.903 * 1050 - 0.897 * 6000 + 3760.898 * 1948 + 1.476 * 6306
```

```
## [1] 265360.4
```

--

The predicted sale price for a house in Levittown, NY with 3 bedrooms, 1 bathroom, 1050 square feet of living area, 6000 square foot lot size, built in 1948 with $6306 in property taxes is **$265,360**.

---

### Predictions

- Just like with simple linear regression, we can use the .vocab[`predict`] function in R to calculate the appropriate intervals for our predicted values 


```r
x0 &lt;- data.frame(bedrooms = 3, bathrooms = 1, living_area = 1050, 
                 lot_size = 6000, year_built = 1948, property_tax = 6306)
predict(price_model, x0, interval = "prediction")
```

--

IN CLASS ACtIVITY - COMPARE INtERVALS AND EXTRAPOLATION

---

### Cautions

- .vocab3[Do not extrapolate!] Because there are multiple explanatory variables, you can extrapolation in many ways

--

- The multiple regression model only shows .vocab3[association, not causality]
  + To show causality, you must have a carefully designed experiment or carefully account for confounding variables in an observational study
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": "%current%"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
