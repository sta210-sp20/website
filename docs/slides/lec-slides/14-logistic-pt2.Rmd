---
title: "Logistic regression"
subtitle: "Model Predictions & Assumptions"
author: "Prof. Maria Tackett"
date: "03.30.20"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta210-slides.css"
    logo: img/sta210-sticker-icon.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%"
---

```{r setup, include=FALSE, echo=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
	fig.align = "center",
	fig.height =5,
	fig.width = 8,
	message = FALSE,
	warning = FALSE
)
```

class: middle, center

### [Click for PDF of slides](14-logistic-pt2.pdf)

---

### Packages

```{r,echo=T}
library(tidyverse)
library(knitr)
library(broom)
library(pROC) #ROC curves
```

---

### Review

- $y$: binary response
  + 1: yes
  + 0: no
  
- $Mean(y) = \pi$

- $Var(y) = \pi(1-\pi)$

- **Odds of "yes"**: $\omega = \frac{\pi}{1-\pi}$

---

### Logistic Regression Model 

- Suppose $P(y_i=1|x_i) = \pi_i$ and $P(y_i=0|x_i) = 1-\pi_i$

- The <font class="vocab">logistic regression model </font> is

$$\log\Big(\frac{\pi_i}{1-\pi_i}\Big) = \beta_0 + \beta_1 x_i$$
<br> 


- $\log\Big(\frac{\pi_i}{1-\pi_i}\Big)$ is called the <font class="vocab">logit</font> function

---

### Risk of coronary heart disease 

This data is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The goal is to predict whether a patient has a 10-year risk of future coronary heart disease.

**Response**: 

.vocab[`TenYearCHD`]: 
- 0 = Patient doesnâ€™t have 10-year risk of future coronary heart disease
- 1 = Patient has 10-year risk of future coronary heart disease

**Predictor**: 

- .vocab[`age`]: Age at exam time.
- .vocab[`currentSmoker`]: 0 = nonsmoker; 1 = smoker
- .vocab[`totChol`]: total cholesterol (mg/dL)


```{r echo = F}
heart_data <- read_csv("data/framingham.csv") %>%
  drop_na() %>%
  mutate(currentSmoker = as.factor(currentSmoker), 
         TenYearCHD = as.factor(TenYearCHD), 
         ageCent = age - mean(age), 
         totCholCent = totChol - mean(totChol))
```

---

### Modeling risk of coronary heart disease

```{r}
risk_m <- glm(TenYearCHD ~ ageCent + currentSmoker + totCholCent, 
              data = heart_data, family = binomial)
tidy(risk_m, conf.int = TRUE) %>% 
  kable(format = "markdown", digits = 3)
```

$$\small{\log\Big(\frac{\hat{\pi}}{1-\hat{\pi}}\Big) = -2.111 + 0.081 \text{ageCent} + 0.447 \text{currentSmoker} + 0.003 \text{totCholCent}}$$

---

class: middle, center

### Part 1: Predictions & Model Fit

---

### Using the model for prediction

- We are often interested in predicting whether a given observation will have a "yes" response 

--

- To do so
  - We use the logistic regression model to find the predicted log-odds that an observation has a "yes" response.
  - Then use the log-odds to calculate the predicted probability of a "yes" response.
  - Then use the predicted probabilities to classify the observation as having a "yes" or "no" response

---

### Calculating the predicted probability

.alert[
$$\hat{\pi}_i = \frac{\exp\{\hat{\beta}_0 + \hat{\beta}_1 x_i\}}{1 + \exp\{\hat{\beta}_0 + \hat{\beta}_1 x_i\}}$$
]

--

$$\log\Big(\frac{\hat{\pi}_i}{1-\hat{\pi}_i}\Big) = \hat{\beta}_0 + \hat{\beta}_1 x_i$$

--

$$\Rightarrow \exp\bigg\{\log\Big(\frac{\hat{\pi}_i}{1-\hat{\pi}_i}\Big)\bigg\} = \exp\{\hat{\beta}_0 + \hat{\beta}_1 x_i\}$$
--

$$\Rightarrow \frac{\hat{\pi}_i}{1-\hat{\pi}_i} = \exp\{\hat{\beta}_0 + \hat{\beta}_1 x_i\}$$
--

$$\Rightarrow \hat{\pi}_i = \frac{\exp\{\hat{\beta}_0 + \hat{\beta}_1 X_i\}}{1+\exp\{\hat{\beta}_0 + \hat{\beta}_1 x_i\}}$$
---

### $\hat{\pi}$ vs. $\widehat{\text{log-odds}}$

$$\hat{\pi}_i = \frac{\exp\{\hat{\beta}_0 + \hat{\beta}_1 x_i\}}{1 + \exp\{\hat{\beta}_0 + \hat{\beta}_1 x_i\}} = \frac{\exp\{\widehat{\text{log-odds}}\}}{1 + \exp\{\widehat{\text{log-odds}}\}} $$

```{r echo = F}

z <- seq(-5, 5, by = 0.5)
pi <- exp(z)/(1 + exp(z))

prob_ex <- data.frame(cbind(z, pi))
ggplot(data = prob_ex, aes(x = z, y = pi)) +
  geom_point() + 
  geom_smooth(method = "glm", 
    method.args = list(family = "binomial"), 
    se = FALSE) +
  labs(x = "Predicted log-odds", 
       y = "Predicted probability", 
       title = "Predicted Probability vs. Predicted Log-Odds")
```


---

### Predicted response for a patient

- Suppose a patient comes in who is 60 years old, does not currently smoke, and has a total cholesterol of 263 mg/dL.

--

- Predicted log-odds that this person is high risk for coronary heart disease in the next 10 years: 
$$\small{\log\Big(\frac{\hat{\pi}}{1-\hat{\pi}}\Big) = -2.111 + 0.081 \text{ageCent} + 0.447 \text{currentSmoker} + 0.003 \text{totCholCent}}$$

--

$$\begin{aligned}\log\bigg(\frac{\hat{\pi}_i}{1-\hat{\pi}_i}\bigg) = &-2.111 + 0.081\times(60 - 49.552) + 0.447 \times 0 \\
&+ 0.003 \times (263 - 236.848)\approx -1.186 \end{aligned}$$

--

- The probability this patient is high risk for coronary heart disease in the next 10 years: 

$$\hat{\pi}_i = \frac{\exp\{-1.186\}}{1 +  \exp\{-1.186\}} = 0.234$$

---

### Predictions in R

```{r}
x0 <- data_frame(ageCent = (60 - 49.552), totCholCent = (263 - 236.848), 
currentSmoker = as.factor(0))
```

- **Predicted log-odds**

```{r}
predict(risk_m, x0) 
```

- **Predicted probabilities**

```{r}
predict(risk_m, x0, type = "response") 
```

---

### Is this patient high risk?


```{r}
predict(risk_m, x0, type = "response") 
```

The probability the patient is at risk for coronary heart disease is `r round(predict(risk_m, x0, type = "response"), 3)`.

--

.question[
Based on this probability, would you consider this patient as being high risk for getting coronary heart disease in the next 10 years? Why or why not?
]

---

### Confusion Matrix

- We can use the predicted probability to predict the outcome for a given observation
  - In other words, we can classify the observations into two groups: "yes" and "no"

--

- **How**: Establish a threshold such that $y=1$ if predicted probability is greater than the threshold ($y=0$ otherwise)

--

- To assess the accuracy of our predictions, we can make a table of the observed (actual) response versus the predicted response.
  + This table is the .vocab[confusion matrix]

--

- We can use this table to calculate the proportion of observations that were misclassifed. This is the .vocab[misclassification rate]. 

---

### Confusion Matrix

Suppose we use 0.3 as the threshold to classify observations

```{r}
threshold <- 0.3
risk_m_aug <- augment(risk_m, type.predict = "response")
```

```{r}
risk_m_aug %>%
  mutate(risk_predict = if_else(.fitted > threshold, "Yes", "No")) %>%
  group_by(TenYearCHD, risk_predict) %>%
  summarise(n = n()) %>%
  kable(format="markdown")
```

---

### Confusion matrix

```{r echo = F}
risk_m_aug %>%
  mutate(risk_predict = if_else(.fitted > threshold, "Yes", "No")) %>%
  group_by(TenYearCHD, risk_predict) %>%
  summarise(n = n()) %>%
  kable(format="markdown")
```
<br><br>

.question[ 
What proportion of observations were misclassified?

What is the disadvantage of relying on the confusion matrix to assess the accuracy of the model?
]

---

### Confusion matrix: 2 X 2 table 

In practice, you often see the confusion matrix presented as a 2 $\times$ 2 table as shown below: 

```{r}
risk_m_aug %>%
  mutate(risk_predict = if_else(.fitted > threshold, "Yes", "No")) %>%
  group_by(TenYearCHD, risk_predict) %>%
  summarise(n = n()) %>%
  spread(risk_predict, n) %>%
  kable(format="markdown")
```

---

### ROC curve: considering many thresholds

```{r, fig.height = 4}
library(plotROC) #extension of ggplot2
(roc_curve <- ggplot(risk_m_aug, 
                     aes(d = as.numeric(TenYearCHD) - 1, 
                         m = .fitted)) +
  geom_roc(n.cuts = 10, labelround = 3) + 
  geom_abline(intercept = 0) + 
  labs(x = "False Positive Rate (1 - Specificity)", 
       y = "True Positive Rate (Sensitivity)") )
```

---

### Sensitivity & Specificity

- <font class="vocab">Sensitivity: </font>Proportion of observations with $y=1$ that have predicted probability above a specified threshold
  + Called true positive rate

- <font class="vocab">Specificity: </font>Proportion of observations with $y=0$ that have predicted probability below a specified threshold
  + (1 - specificity) called **false positive rate**

--

- What we want: 
  + High sensitivity
  + Low values of 1-specificity

---

### ROC Curve

- <font class="vocab">Receive Operating Characteristic (ROC) curve </font>: 
  + *x-axis*: False positive rate (1 - Specificity)
  + *y-axis*: True positive rate (Sensitivity)
 
--
 
- We can think of each point on the curve as representing a confusion matrix at a given threshold

--

- We can use the area under the curve (AUC) as one way to assess how well the logistic model fits the data
  - $AUC = 0.5$ very bad fit (no better than a coin flip)
  - $AUC$ close to 1: good fit
  
---

### Area under curve

```{r echo = F}
roc_curve
```

```{r}
calc_auc(roc_curve)$AUC
```

---

### Which threshold would you choose?

.question[
A doctor plans to use the results from your model to help select patients for a new heart disease prevention program. She asks you which threshold would be best to select patients for this program. Based on the ROC curve from the previous slide, which threshold would you recommend to the doctor? Why?
]


---

class: middle, center

### Part 2: Checking Assumptions 

---

### Assumptions for logistic regression 

We want to check the following assumptions for the logistic regression model: 

1. .vocab[Linearity]: Is there a linear relationship between the log-odds and the predictor variables 

<br>

2. .vocab[Randomness]: Was the sample randomly selected? Or can we reasonably treat it as random? 

<br>

3. .vocab[Independence]: The outcome of one observation does not affect the outcome of another observation

---

### Linearity: binned residual plots

- It is not useful to plot the raw residuals, so we will examine binned residual plots 

**When examining binned residuals**
    
- Plot should have no discernible pattern or trend
  - Nonlinear trend may be indication that squared term or log transformation of predictor variable required

- If bins have average residuals with large magnitude
  + Look at averages of other predictor variables across bins
  + Interaction may be required if large magnitude residuals correspond to certain combinations of predictor variables
  
---

### Binned plot vs. predicted values

- Use the <font class="vocab">`binnedplot`</font> function in the **arm** package. 
    - *Tip: Don't load the **arm** package to avoid conflicts with tidyverse*
    
.small[
```{r}
arm::binnedplot(x = risk_m_aug$.fitted, y = risk_m_aug$.resid,
                xlab = "Predicted Probabilities", 
                main = "Binned Residual vs. Predicted Values", 
                col.int = FALSE)
```
]

---

### Making binned residual plot

- Calculate raw residuals

- Order observations either by the values of the predicted probabilities (or by numeric predictor variable)

- Use the ordered data to create *g* bins of approximately equal size. Default value: $g = \sqrt{n}$

- Calculate average residual value in each bin

- Plot average residuals vs. average predicted probability (or average predictor value)

---

###  Residuals vs. `Age`

Make binned plot with predictor on $x$ axis

```{r}
arm::binnedplot(x = risk_m_aug$ageCent, 
                y = risk_m_aug$.resid, 
                col.int = FALSE,
                xlab = "Age (Mean-Centered)", 
                main = "Binned Residual vs. Age")
```

---

### Residuals vs. `totChol`


```{r}
arm::binnedplot(x = risk_m_aug$totCholCent, 
                y = risk_m_aug$.resid, 
                col.int = FALSE,
                xlab = "Total Cholesterol (Mean-Centered)", 
                main = "Binned Residual vs. Total Cholesterol")
```

---

### Residuals vs. categorical predictors 

- Calculate average residual for each level of the predictor
    - Are all means close to 0? If not, there is a problem with model fit.
    
```{r}
risk_m_aug %>%
  group_by(currentSmoker) %>%
  summarise(mean_resid = mean(.resid))
```

---

### Randomness

Assess randomness based on a description of the data collection

- Was the sample randomly selected?
- If the sample was not randomly selected, is there reason to believe the observations in the sample differ systematically from the population of interest? 

.question[
What do you conclude about the randomness assumption for our dataset?
]

---

### Independence

Assess randomness based on a description of the data collection

- Does the outcome of one observation affect the outcome of the other? 
  - This assumption is most often violated when data was collected over time or there is a spatial relationship between observations? 
  
.question[
What do you conclude about the randomness assumption for our dataset?
]





