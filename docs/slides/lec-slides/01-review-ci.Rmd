---
title: "Inference Review"
subtitle: "Confidence Intervals"
author: "Prof. Maria Tackett"
date: "01.13.20"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta210-slides.css"
    logo: img/sta210-sticker-icon.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%"
---

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
library(knitr)
opts_chunk$set(fig.height = 2.65, dpi = 300) 
# ggplot2 color palette with gray
color_palette <- list(gray = "#999999", 
                      salmon = "#E69F00", 
                      lightblue = "#56B4E9", 
                      green = "#009E73", 
                      yellow = "#F0E442", 
                      darkblue = "#0072B2", 
                      red = "#D55E00", 
                      purple = "#CC79A7")

options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
	fig.align = "center",
	fig.height = 3.5,
	fig.width = 6.25,
	message = FALSE,
	warning = FALSE
)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(gridExtra)
library(infer)
```

class: middle, center

### [Click for PDF of slides](01-review-ci.pdf)

---

## Announcements

- Complete [surveys and consent form](https://www2.stat.duke.edu/courses/Spring20/sta210.001/misc/jan8.html) by Wed at 11:59p

- [Reading for today & Wednesday](https://www2.stat.duke.edu/courses/Spring20/sta210.001/reading/reading-01.html)

- Office hours start today. See [course homepage](https://www2.stat.duke.edu/courses/Spring20/sta210.001/) for schedule

- **New to R or need a refresher?**
  - Attend an Intro to R workshop 
  - Offered Jan 13 - 16, 6p - 7:30p, Gross Hall 270 (choose 1 night to attend)
  - [Click here](https://forms.office.com/Pages/ResponsePage.aspx?id=TsVyyzFKnk2xSh6jbfrJTFvPd3eQtCxBtEvbtkDole1UNjYyVzVTSFdYMVJaWDBXUEhQS0tXWE81VC4u) to sign up
  
- Find more info about statistics related events on [Sakai](https://sakai.duke.edu)

---

class: middle, center

### Any questions from last class?

---

### Today's Agenda

- Sampling distributions & the Central Limit Theorem

- Calculating confidence intervals

---

### *Sesame Street*

- *Sesame Street* is a television series designed to teach children ages 3-5 basic educaitonal skills such as reading (e.g. the alphabet) and math (e.g. counting)

- Tody we are going to analyze data from an [study conducted by the Educational Testing Service](http://files.eric.ed.gov/fulltext/ED122799.pdf) in the early 1970s to test the  effectiveness of the program. 

```{r fig.align="center",out.width="50%",echo=FALSE}
knitr::include_graphics("img/01/sesame_street.jpg")
```
---

### *Sesame Street* study

- Children from 6 locations around the United States (including Durham!) participated in the study. The children were split into two groups (`treatment`):
      + **Group 1**: Those who were encouraged to watch the show (assume watched regularly)
      + **Group 2**: Those who didn't get encouragement to watch the show (assume didn't watch regularly)

- Each child was given a test before and after the study to measure their knowledge of basic math, reading, etc.

- We will focus on the change in reading (identifying letters) scores (`change`)
<br><br>

<small>
[Sesame Street Data - Full Description](http://www2.stat.duke.edu/~jerry/sta210/sesamelab.html) 
Original Study: *Ann Bogatz, Gerry & Ball, Samuel. (1971). The Second Year of Sesame Street: A Continuing Evaluation. Volume 1. vols. 1 & 2.*
</small>

---

### Let's look at the data

`sesame_street.csv` is available in the datasets repo on GitHub.

```{r, echo = F}
sesame_street <- read_csv("data/sesame_street.csv")
```

```{r}
sesame_street %>%
  slice(1:10)
```

---

### Exploratory Data Analysis - Univariate

<small>
```{r}
ggplot(data = sesame_street, mapping = aes(x = change)) +
  geom_histogram(fill = "steelblue", color = "black") +
  labs(x = "Change in Reading Score (Post - Pre)" , 
       title = "Distribution of the Change in Reading Scores")
```
</small>

---

### Exploratory Data Analysis - Univariate

- Calculate summary statistics for `change`
<small>
```{r}
sesame_street %>%
  summarise(n = n(), min = min(change), median = median(change), max = max(change), 
            IQR = IQR(change), 
            mean = mean(change), std_dev = sd(change))
```
</small>

---

### Exploratory Data Analysis - Bivariate

<small>
```{r fig.height = 3}
ggplot(data = sesame_street, 
       mapping = aes(y = change, x = treatment)) +
  geom_boxplot(fill = "steelblue") +
  labs(x = "Change in Reading Score (Post - Pre)", 
         title = "Distribution of the Change in Reading Score", 
       subtitle = "Encouraged vs. Not Encouraged.")
```
</small>

---

### Exploratory Data Analysis - Bivariate

Calculate summary statistics for `change` for each group of `treatment`
<small>
```{r}
sesame_street %>%
  group_by(treatment) %>%
  summarise(n = n(), min = min(change), median = median(change), 
            max = max(change), IQR = IQR(change), 
            mean = mean(change), std_dev = sd(change))
```

---

### What is statistical inference?

.pull-left[

```{r fig.align="center",out.width="80%",echo=FALSE}
knitr::include_graphics("img/01/pop_samp.jpg")
```
]

.pull-left[
- <font class = "vocab">Statistical inference</font> is the process of using sample data to make conclusions about the underlying population from which the sample was taken

- Types of inference:
  - <font class="vocab">Confidence Intervals: </font> Estimate the parameter of interest
  - <font class="vocab">Hypothesis Tests: </font> Test a specified claim or hypothesis
]

<font size = "3">
Image source: https://keydifferences.com/difference-between-population-and-sample.html
</font>
---

class: middle, center

### Confidence Interval for a Population Mean

---

### Confidence Intervals

- Developed by Jerzy Neyman (in the 1930s)

- <font class="vocab">**What**</font>: Plausible range of values for a population parameter
  + Assuming sample data is a random sample from the population
  
- <font class="vocab">**Why**</font>: Because the statistic is a random variable, its value is subject to chance error, i.e. random variability
  + We want to take that variability into account by reporting a range of plausible values the parameter can take rather than solely relying on a single statistic

---

### Let's think about the *Sesame Street* data

- We want to know the true mean change in reading scores for all children in the U.S. ages 3 - 5 after 26 weeks (the length of the study). This is the .vocab[population parameter].

<br><br>
--

- We aren't able to collect data on all children in the U.S. ages 3 - 5, but we do have data on 240 children who particpated in the study. This is the .vocab[sample statistic].

---

### Let's think about the *Sesame Street* data


- Our best guess for the true mean change in reading scores is the mean change in reading scores from our sample: 10.8

<br><br>
--

- If we redid the study using 240 different children, we'd expect the mean change in reading score in that sample to be differ from 10.8. This is .vocab[sampling variability]. 

<br><br>
--

- Our goal, then is to account for that sampling variablity and calculate a plausible range of values the true mean can take.


---

### Sampling distribution

- A .vocab[sampling distribution] is the distribution of sample statistics from random samples of of the same size taken with replacement from a population

- In practice it is impossible to construction sampling distributions,  since it would 
require having access to the entire population. However, we have theorems that tell us what the sampling distribution will look like (more on this shortly.)

--

- For now, let's do a couple of demonstrations to get an idea about some basic properties of sampling distributions. 
    - For the demonstration, we will make the unrealistic assumption that we have access ot the population and will construct the sample distribution. 
    
---

### The population

```{r}
set.seed(011320)
norm_pop <- tibble(x = rnorm(n = 100000, mean = 20, sd = 3))
ggplot(data = norm_pop, aes(x = x)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Population distribution")
```

---

### Sampling from the population - 1

```{r}
samp_1 <- norm_pop %>%
  sample_n(size = 50, replace = TRUE)
```

--

```{r}
samp_1 %>%
  summarise(x_bar = mean(x))
```

---

### Sampling from the population - 2

```{r}
samp_2 <- norm_pop %>%
  sample_n(size = 50, replace = TRUE)
```

--

```{r}
samp_2 %>%
  summarise(x_bar = mean(x))
```


---

### Sampling from the population - 3

```{r}
samp_3 <- norm_pop %>%
  sample_n(size = 50, replace = TRUE)
```

--

```{r}
samp_3 %>%
  summarise(x_bar = mean(x))
```


--

.vocab[keep repeating...]

---

### Population vs. sampling distributions

```{r echo = F}
sampling <- norm_pop %>%
  rep_sample_n(size = 50, replace = TRUE, reps = 1000) %>%
  group_by(replicate) %>%
  summarise(xbar = mean(x))
```

```{r echo=FALSE, fig.height= 4}
p1 <- ggplot(data = norm_pop, aes(x = x)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Population distribution") +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank()) + 
  xlim(5, 35)
p2 <- ggplot(data = sampling, aes(x = xbar)) +
  geom_histogram(binwidth = 0.25) +
  labs(title = "Sampling distribution of sample means") +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank()) +
  xlim(5, 35)
grid.arrange(p1, p2, nrow = 2)
```

---

### Discussion 

Take a minute to discuss the following with 1 - 2 people around you: 

.question[
How do the shapes, centers, and spreads of these distributions compare?
]

---

### Let's simulate another distribution

```{r}
rs_pop <- tibble(x = rbeta(100000, 1, 5) * 100)
```

```{r echo=FALSE, fig.height=2}
ggplot(data = rs_pop, aes(x = x)) +
  geom_histogram(binwidth = 5) +
  labs(title = "Population distribution") +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank())
```

```{r echo=FALSE}
rs_pop %>%
  summarise(mu = mean(x), sigma = sd(x))
```

---

### Sampling distribution

```{r}
sampling <- rs_pop %>%
  rep_sample_n(size = 50, replace = TRUE, reps = 1000) %>%
  group_by(replicate) %>%
  summarise(xbar = mean(x))
```

```{r echo=FALSE, fig.height=2}
ggplot(data = sampling, aes(x = xbar)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Sampling distribution of sample means") +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank())
```

```{r echo=FALSE}
sampling %>%
  summarise(mean = mean(xbar), se = sd(xbar))
```

---

### Population vs. sampling distribution 

```{r echo=FALSE, fig.height=3.5}
p1 <- ggplot(data = rs_pop, aes(x = x)) +
  geom_histogram(binwidth = 5) +
  labs(title = "Population distribution") +
  xlim(-5, 100) +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank())
p2 <- ggplot(data = sampling, aes(x = xbar)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Sampling distribution of sample means") +
  xlim(-5, 100) +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank())
grid.arrange(p1, p2, nrow = 2)
```

---

### In-class exercise 

.question[

- Use the two examples we just discussed to answer the questions: http://bit.ly/sta210-sp20-samp 

- Use **NetId@duke.edu** for your email address.

- You are welcome (and encouraged!) to discuss these questions with 1 - 2 people around you, but **each person** must submit a response.
]

---

### Central Limit Theorem

- Using the .vocab[Central Limit Theorem (CLT)] we know the form of the sampling distribution for certain statistics such as the mean, proportion, difference in means, etc.
    - CLT does not apply to all statistics (e.g. the median)

- By the Central Limit Theorem, when the conditions are met, we know the sampling distribution of the sample statistic will..
  - be approximately Normal
  - have a mean equal to the unknown population parameter
  - have a standard deviation proportional to the inverse of the square root of the sample size.

- Get more details on the derivation of the CLT in STA 240 & STA 250

---

### CLT for a population mean 

.alert[
Suppose we have a population with mean $\mu$ and standard deviation $\sigma$. By the CLT, when conditions are met, the sampling distribution of the sample mean is 

$$\bar{x} \sim N\Big(\mu, \frac{\sigma}{\sqrt{n}}\Big)$$
]

<br>

**Conditions:** 
- **Independence**: The sampled observations must be independent. This is difficult to check, but the following are useful guidelines:
    - the sample must be random
    - if sampling without replacement, sample size must be less than 10% of the population size

- **Sample Size**: Sample size is large. Usually $n > 30$ is considered large enough sample. Need larger sample size if population distribution is extremely skewed. 

- If comparing two populations, the groups must be independent of each other,
and all conditions should be checked for both groups.

---

## Standard Error

- By the CLT, the standard deviation of the sampling distribution of $\bar{x}$ is $\sigma/ \sqrt{n}$. 

- In practice, we don't know the population standard deviation $\sigma$, but we can estimate it using the sample standard deviation $s$. 

- The <font class="vocab">standard error (SE) </font> is the *standard deviation* of the *sampling distribution*, calculated using sample statistics

.alert[
$$SE = \frac{s}{\sqrt{n}}$$
]

- We will use the standard error for calculations of confidence intervals and hypothesis tests

---

### Confidence interval for the mean

The $C%$ confidence interval to estimate $\mu$ is 

.alert[
$$\bar{x} \pm t_{df}^* \times \frac{s}{\sqrt{n}}$$
where $t_{df}^*$ is the critical value calculated from the $t$ distribution with $n-1$ degrees of freedom. 
]

---

## t-distribution vs. Normal 

- We need to account for the extra variability that comes from using $s/\sqrt{n}$ (instead $\sigma/\sqrt{n}$) Therefore, we will use the $t$ distribution for the shape of the sampling distribution of $\bar{x}$ in our calculations.

```{r, echo=FALSE,out.width = '70%'}
knitr::include_graphics("img/02/tdistribution.png")
```

<font size="2">Picture from <i>The Basic Practice of Statistics (7th edition)</i></font>

---

### *Sesame Street* data 

Let's calculate the 95\% confidence interval for the mean change in reading scores in 26 weeks. 

```{r echo = F}
sesame_street %>%
  summarise(n = n(), mean = mean(change), std_dev = sd(change))
```

```{r}
(t_star <- qt(0.975, 239))
```

---


### Confidence interval for the difference in means

The $C%$ confidence interval to estimate $\bar{\mu}_1 - \bar{\mu}_2$ is 

$$(\bar{x}_1 - \bar{x}_2) \pm t_{df}^* \times \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}$$
where $t_{df}^*$ is the critical value calculated from the *t* distribution with *df* degrees of freedom

---

```{r}
sesame_street %>%
  group_by(treatment) %>%
  summarise(n = n(), mean = mean(change), sd = sd(change))
```

- **Parameter**: $\mu_{e} - \mu_{ne}$
- **Statistic**: $\bar{x}_{e} - \bar{x}_{ne}$

- In the last class, we conducted a hypothesis test and came to the conclusion that children who watched *Sesame Street* regularly showed greater improvement in reading scores, on average, than children who didn't want the show regularly.  

.alert[
Today we will estimate the difference in average improvement between the two groups, i.e. estimate $\mu_{e} - \mu_{ne}$. 
]



---

### Deriving the confidence interval

- In the *Sesame Street* example, the parameter of interest is the difference in means, $\mu_{1} - \mu_{2}$. Let's look at the confidence interval for $\mu_{1} - \mu_{2}$ based on the CLT

--

- The statistic is the difference in sample means $\bar{x}_1 - \bar{x}_2$

- Assuming the conditions for the CLT are met (independent observations and large $n$), the sampling distribution for $\bar{x}_1 - \bar{x}_2$ is 

$$\bar{x}_1 - \bar{x}_2 \sim N \Bigg(\mu_1 - \mu_2, \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}\Bigg)$$
---

### Deriving the confidence interval

- By the CLT and properties of the Normal distribution, in 95% of random samples, 

<small>
$$(\mu_1 - \mu_2) - 1.96 \times \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}} \leq \bar{x}_1 - \bar{x}_2 \leq (\mu_1 - \mu_2) + 1.96 \times \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}$$
</small>

--


- Now, let's center the inequality around the parameter $\mu_1 - \mu_2$

<small>
$$(\bar{x}_1 - \bar{x}_2) - 1.96 \times \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}} \leq \mu_1 - \mu_2 \leq (\bar{x}_1 - \bar{x}_2) + 1.96 \times \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}$$
</small>

--
.alert[
Range of plausible values for $\mu_1 - \mu_2$ (using 95% confidence)
$$(\bar{x}_1 - \bar{x}_2) \pm 1.96 \times \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}$$
]
---


## General form of the CI

- Generalizing the equations on the previous slide, all confidence intervals take the form $[LB, UB]$

$$\text{Lower Bound (LB)} = \text{ Estimate} - \text{ (critical value) } \times SE$$
$$\text{Upper Bound (UB)} = \text{ Estimate} + \text{ (critical value) } \times SE$$
--

- Let's talk about the <font class="vocab">*SE*</font> and the critical value
---

## Standard Error of $\bar{x}_1 - \bar{x}_2$

- In practice, we don't know the population standard deviations $\sigma_1$ and $\sigma_2$

--

- We will use the sample standard deviations $s_1$ and $s_2$ to estimate $\sigma_1$ and $\sigma_2$

--

- Thus, the <font class="vocab">**standard error of $\mathbf{\bar{x}_1 - \bar{x}_2}$**</font> is

$$\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}$$

---

## t-distribution vs. Normal 

- We need to account for the extra variability that comes from using $s_1$ and $s_2$ (instead of $\sigma_1$ and $\sigma_2$). Therefore, we will use the *t* distribution for sampling distribution of $\bar{x}_1 - \bar{x}_2$ 

```{r, echo=FALSE,out.width = '70%'}
knitr::include_graphics("img/02/tdistribution.png")
```

<font size="2">Picture from <i>The Basic Practice of Statistics (7th edition)</i></font>

---

### Confidence interval for the difference in means

The $C%$ confidence interval to estimate $\bar{\mu}_1 - \bar{\mu}_2$ is 

$$(\bar{x}_1 - \bar{x}_2) \pm t_{df}^* \times \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}$$
where $t_{df}^*$ is the critical value calculated from the *t* distribution with *df* degrees of freedom

---

## Calculating the critical value 

The critical value, $t^*$, follows a $t$ distribution with degrees of freedom given by the formula:
<br>
<br>

$$df = \frac{\Big(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}\Big)^2}{\frac{1}{n_1-1}\Big(\frac{s_1^2}{n_1}\Big)^2 + \frac{1}{n_2-1}\Big(\frac{s_2^2}{n_2}\Big)^2} \approx min\{n_1-1,n_2-1\}$$
<br>
<br>
In practice, we will use R to calculate the degrees of freedom.

---

### Understanding a 95% Confidence Interval

<div align="center">
```{r, echo=FALSE,out.width = '50%'}
knitr::include_graphics("img/02/confidence_intervals.png")
```
</div>

- The goal is to produce an interval for the parameter of interest using statistics calculated from a random sample

- If we repeated this process thousands of times, we would expect about 95% of the intervals to contain the true parameter of interest

- Note this is <u>**not**</u> the same as saying there's a 95% probability that the parameter is in a given interval 

---

## *Sesame Street* Example 

```{r echo = F}
treatment <- sesame_street %>% filter(treatment == "Encouraged")
control <- sesame_street %>% filter(treatment == "Not Encouraged")
mean1 <- mean(treatment$change)
mean2 <- mean(control$change)
stat <-  mean1 - mean2

var1 <- var(treatment$change)
var2 <- var(control$change)
n1 <- nrow(treatment)
n2 <- nrow(control)
se <- sqrt(var1 / n1 + var2 / n2)

df = (var1/n1 + var2/n2)^2/((var1/n1)^2*(n1-1)^(-1) + (var2/n2)^2*(n2-1)^(-1))

t <- qt(0.975, df)
LB <- stat - t * se
UB <- stat + t * se
```

The 95% confidence interval to estimate the mean difference in reading score improvement between children who watched *Sesame Street* regularly versus those who didn't is 

<center><large>
**[`r round(LB, 3)`, `r round(UB, 3)`]**
</center>
</large>

<br><br>

.question[
1. Interpret this interval in context. 

2. Based on this interval, what do you conclude about the effectiveness of *Sesame Street*?
]

---

### Practice (OpenIntro 5.25)

```{r,echo=F,results=F}
mean = c(35749,35500)
sd = c(39421,41512)
n = c(21,17)
city = c("Cleveland, OH", "Sacramento, CA")
data = as.data.frame(cbind(city,mean,sd,n))
```

Average income varies from one region of the country to
another, and it often reflects both lifestyles and regional living expenses. 
<br>
<br>
Suppose a new graduate is considering a job in two locations, Cleveland, OH and Sacramento, CA, and he wants to see whether the average income in one of these cities is higher than the other. The summary data is show below:
.pull-left[
```{r, echo=FALSE,out.width = '100%'}
knitr::include_graphics("img/02/income.png")
```
]

.pull-right[
```{r,echo=F}
knitr::kable(data,format="html")
```
]
```{r,echo=F,results=F}
se = sqrt(sd[1]^2/n[1] + sd[2]^2/n[2])
estimate = mean[1] - mean[2]
t.crit = qt(0.975,16)
LB = estimate - t.crit*estimate
UB = estimate + t.crit*estimate
```

---

### Practice: (OpenIntro 5.25)

The 95% confidence interval for the difference in the mean income between Cleveland and Sacramento is 
```{r,echo=F}
knitr::kable(cbind(LB,UB),format="html")
```

--
<br> 

.question[
1. Interpret the interval iin context. 

2. How would the interval change if we increased the confidence level to 99%?

3. Why might any conclusions based on this interval be unreliable?
]

---

### Confidence intervals and hypothesis tests

- Confidence intervals can be used to assess a hypothesis or claim about a population parameter

- Suppose $\mu$ is the parameter of interest and you calculate a 95\% confidence interval  

- Let's also suppose that the hypotheses are $H_0: \mu = 1$ vs. $H_a: \mu \neq 1$
    - If the 95% confidence interval contains 1, then this two-sided hypothesis test will result in a p-value that is greater than 0.05
    - If the 95% confidence interval does not contain 1, then this two-sided hypothesis test will result in a p-value that is less than 0.05

---

## Practice 

.instructions[
Determine if each state is true or false. If it is false, rewrite the statement so it is true.
]

1. If you increase sample size, the width of confidence intervals will increase.

2. For a given standard error, higher confidence levels (e.g. 99% vs. 95%) result in wider confidence intervals.


3. The statement, "the p-value is .003", is equivalent to the statement, "there is a 0.3% probability that the null hypothesis is true".

4. A p-value of .04 is more evidence against the null hypothesis than a p-value of .08.



---


class: middle, center 

### Understanding the Hypothesis Test

---

## Calculating the p-value

- <font class="vocab3">p-value:</font> probability of getting a test statistic as extreme or more extreme than the calculated test statistic, assuming the null hypothesis is true
--


- When the alternative has a $>$, the p-value is calculated using the area to the right of the test statistic
--


- When the alternative has a $<$, the p-value is calculated using the area to the left of the test statistic
--


- When the alternative has $\neq$, the p-value is calculated as the area to the left of $-|\text{test statistic}|$ and to the right of $|\text{test statistic}|$

---

### Interpreting the p-value
--

**What the p-value is NOT**:
- It is <u>*not*</u> the probability the null hypothesis is true
  + The null hypothesis is either true or not true
- (1 - *p-value*) is <u>*not*</u> the probability that the alternative hypothesis is true
  + The alternative hypothesis is either true or not true

--

.alert[
The p-value **IS**

The probability of getting a test statistic as extreme or more extreme than the calculated test statistic, *assuming the null hypothesis is true.*
]

---

## Interpreting the p-value

|  Magnitude of p-value |             Interpretation            |
|:---------------------:|:-------------------------------------:|
| p-value < 0.01        | strong evidence against $H_0$         |
| 0.01 < p-value < 0.05 | moderate evidence against $H_0$       |
| 0.05 < p-value < 0.1  | weak evidence against $H_0$           |
| p-value > 0.1         | effectively no evidence against $H_0$ |
<br> 
<br>

**Note:** These are general guidelines. The strength of evidence depends on the context of the problem.
---

## Statistical Significance

- A threshold can be used to decide whether or not to reject $H_0$. 

- This threshold is called the <font class="vocab3">significance level</font> and is usually denoted by $\alpha$

- When $H_0$ is rejected, we use the term <font class="vocab3"> statistically significant </font> to describe the outcome of the test.

- *Example*: When $\alpha = 0.05$, results are statistically significance when the p-value is $< 0.05$

---

## Statistical Significance

- Do not rely strictly on the significance level to make a conclusion!
--

- Suppose the significance level is 0.05
--

  + If the p-value is 0.05001, we do not reject $H_0$
--

  + If the p-value is 0.04999, we do reject $H_0$
--

- p-values of 0.05001 and 0.04999 are practically the same, yet they lead to different conclusions. 
--

- Always state the p-value when reporting results and assess it's magnitude in the context of your problem. 
---

class: results 

### Results that Aren't Statistically Significant

- An outcome of failing to reject $H_0$ is <u>*not*</u> a failed study/experiment

- Obtaining an outcome of "no significant effect" or "no significant difference" is still valid 

- It is often just as important to learn that the $H_0$ can't be refuted

---

## Type I &  Type II Errors

<center>
```{r, echo=FALSE,out.width = '80%'}
knitr::include_graphics("img/02/errors.png")
```
<small>Image: <i>The Basic Practice of Statistics (7th Ed.)</i></small>
</center>
<br> 

- <font class="vocab3">Type I Error</font>: Reject $H_0$ when $H_0$ is true
- <font class="vocab3">Type II Error</font>: Fail to reject $H_0$ when $H_1$ is true
- Replicate study when possible to reduce these errors
<br>
<br>

---

## Reducing Error

- Probability of Type I error is the significance level, i.e the threshold for rejecting $H_0$

- Probability of Type II error decreases as the sample size increases
    - When designing a study, it is good practice to conduct a power analyses to determine the sample size required to minimize the chance of Type II error

---

## Practice (OpenIntro 5.29)

A food safety inspector is called upon to investigate a restaurant with a few customer reports of poor sanitation practices. The food safety inspector uses a hypothesis testing framework to evaluate whether regulations are not being met. If he decides the restaurant is in gross violation, its license to serve food will be revoked.


.question[

a. What are the null and alternative hypotheses (in words)?

a. What is a Type 1 Error in this context?

b. What is a Type 2 Error in this context?

d. Which error is more problematic for the diners? Why?
]

---

## Before Next Class

- Fill out the **Getting To Know You Survey on Sakai** - due TODAY at 11:59p

- **New to R or need a refresher?**
  - Duke Libraries Rfun - Intro to R Workshop: Data Transformations, Data Structures, and the Tidyverse
      - September 12 1p - 3p
      - To register: https://duke.libcal.com/event/5497129
    - *Work with Data* primer on RStudio Cloud: [https://rstudio.cloud/learn/primers/2](https://rstudio.cloud/learn/primers/2)
    - "Data Visualization" in *R for Data Science*:
    [https://r4ds.had.co.nz/data-visualisation.html](https://r4ds.had.co.nz/data-visualisation.html)

- **More on statistical inference**
  - [OpenIntro Statistics](https://www.openintro.org/download.php?file=os3_tablet&referrer=/stat/textbook.php) Chapter 5: Inference for numerical data

