<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Multiple Linear Regression</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr. Maria Tackett" />
    <link rel="stylesheet" href="sta210-slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Multiple Linear Regression
## Model Diagnostics
### Dr. Maria Tackett
### 10.02.19

---




class: middle, center

### [Click for PDF of slides](11-model-diagnostics.pdf)

---

### Announcements

- HW 03 **due TODAY at 11:59p**

- HW 04 **due Thursday, October 10 at 11:59p**

- Thursday's lab: Help Hours

- Looking ahead: 
    - **Exam 1 on Mon, Oct 14 in class**
    - Practice exam on Sakai 
    - Can bring 1 page of notes
    - Exam review on Oct 9
    - Lecture notes, past assignments, and textbook to study
---

## R packages


```r
library(tidyverse)
library(knitr)
library(broom)
library(cowplot) # use plot_grid function
```

---

class: middle, center

## Nested F Test

---

## Restaurant tips

What affects the amount customers tip at a restaurant?

- **Response:**
    - &lt;font class="vocab"&gt;`Tip`&lt;/font&gt;: amount of the tip
    
- **Predictors:**
    - &lt;font class="vocab"&gt;`Party`&lt;/font&gt;: number of people in the party
    - &lt;font class="vocab"&gt;`Meal`&lt;/font&gt;:  time of day (Lunch, Dinner, Late Night) 
    - &lt;font class="vocab"&gt;`Age`&lt;/font&gt;: age category of person paying the bill (Yadult, Middle, SenCit)



---

### Is `Meal` a significant predictor of tips?

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.254 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.394 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.182 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.002 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Party &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.808 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.121 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 14.909 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; AgeSenCit &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.390 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.394 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.990 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.324 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; AgeYadult &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.505 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.412 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.227 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.222 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; MealLate Night &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.632 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.407 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -4.013 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; MealLunch &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.612 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.402 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.523 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.130 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

### Tips data:  Nested F Test

`$$\begin{aligned}&amp;H_0: \beta_{late night} = \beta_{lunch} = 0\\
&amp;H_a: \text{ at least one }\beta_j \text{ is not equal to 0}\end{aligned}$$`

--


```r
reduced &lt;- lm(Tip ~ Party + Age, data = tips)
```

--


```r
full &lt;- lm(Tip ~ Party + Age + Meal, data = tips)
```

--


```r
kable(anova(reduced, full), format="markdown", digits = 3)
```



| Res.Df|     RSS| Df| Sum of Sq|     F| Pr(&gt;F)|
|------:|-------:|--:|---------:|-----:|------:|
|    165| 686.444| NA|        NA|    NA|     NA|
|    163| 622.979|  2|    63.465| 8.303|      0|

--

**At least one coefficient associated with `Meal` is not zero. Therefore, `Meal` is a significant predictor of `Tips`.**

---

class: middle

.question[
Why can we not rely on the individual p-values to determine if a categorical variable with `\(k &gt; 2\)` levels) is significant? 

*Hint*: What does it actually mean if none of the `\(k-1\)` p-values are significant?
]

---

### Practice with Interactions 

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.2764989 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.4910882 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.5993270 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0102086 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Party &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.7947980 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.1715003 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 10.4652753 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0000000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; AgeSenCit &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.4007889 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.3969295 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.0097230 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.3141431 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; AgeYadult &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.4701634 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.4197146 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.1201978 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.2642977 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; MealLate Night &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.8454674 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.7089728 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -2.6030159 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0101039 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; MealLunch &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.4608832 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.8651044 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.5327487 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5949421 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Party:MealLate Night &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.1108600 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.2846584 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.3894491 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.6974586 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Party:MealLunch &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.0500822 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.2825586 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.1772455 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.8595384 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

.question[
1. What is the baseline level for `Meal`?
2. How do we expect the mean tips to change when `Meal == "Late Night"`, holding Age and Party constant? 
4. How does the slope of `Party` change when `Meal == "Late Night"`, holding Age and Party constant?
]
---

### Nested F test for interactions

**Are there any significant interaction effects with Party in the model?**


```r
reduced &lt;- lm(Tip ~ Party + Age + Meal, data = tips)
```

--


```r
full &lt;- lm(Tip ~ Party + Age + Meal + Age* Party + Meal * Party, 
           data = tips)
```

--


```r
kable(anova(reduced, full ), format="markdown", digits = 3)
```



| Res.Df|     RSS| Df| Sum of Sq|     F| Pr(&gt;F)|
|------:|-------:|--:|---------:|-----:|------:|
|    163| 622.979| NA|        NA|    NA|     NA|
|    159| 615.380|  4|       7.6| 0.491|  0.742|

---

### Final model for now

We conclude that there are no significant interactions with `Party` in the model. Therefore, we will use the original model that only included main effects. 


&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.254 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.394 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.182 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.002 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Party &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.808 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.121 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 14.909 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; AgeSenCit &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.390 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.394 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.990 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.324 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; AgeYadult &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.505 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.412 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.227 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.222 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; MealLate Night &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.632 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.407 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -4.013 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; MealLunch &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.612 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.402 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.523 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.130 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

class: middle, center

## Model Diagnostics

---

class: middle, center

## Influential and Leverage Points

---

### Influential Observations

An observation is &lt;font class="vocab3"&gt;influential&lt;/font&gt; if removing it substantially changes the coefficients of the regression model 

&lt;img src="11-model-diagnostics_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;

---

### Influential Observations 

- In addition to the coefficients, influential observations can have a large impact on the standard errors

- Occasionally these observations can be identified in the scatterplot
  + This is often not the case - especially when dealing with multivariate data

- We will use measures to quantify an individual observation's influence on the regression model 
  + **leverage**, **standardized residuals**, and **Cook's distance**

---

### Leverage

- &lt;font class="vocab3"&gt;Leverage: &lt;/font&gt; measure of the distance between an observation's values of the predictor variables and the average values of the predictor variables for the whole data set
  
- An observation has high leverage if its combination of values for the predictor variables is very far from the typical combinations in the data 
  + It is **&lt;u&gt;potentially&lt;/u&gt;** an influential point, i.e. may have a large impact on the coefficient estimates and standard errors


- **Note:** Identifying points with high leverage has nothing to do with the values of the response variables

---

### Calculating Leverage

- &lt;font class="vocab"&gt;Simple Regression:&lt;/font&gt; leverage of the `\(i^{th}\)` observation is 
.alert[
`$$h_i =  \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{j=1}^{n}(x_j-\bar{x})^2}$$`
]


- &lt;font class="vocab"&gt;Multiple Regression:&lt;/font&gt; leverage of the `\(i^{th}\)` observation is the `\(i^{th}\)` diagonal of
`$$\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$$`

---

### High Leverage

- Values of leverage are between `\(\frac{1}{n}\)` and 1 for each observation 

- The average leverage for all observations in the data set is `\(\frac{(p+1)}{n}\)`

- There are different thresholds for determining when an observation has **high leverage**
  + We will use the threshold  `\(h_i &gt; \frac{2(p+1)}{n}\)`

- Observations with high leverage tend to have small residuals

---

### High Leverage

- Questions to check if you identify points with high leverage: 
  + Are they a result of data entry errors?
  + Are they in the scope for the individuals for which you want to make predictions?
  + Are they impacting the estimates of the model coefficients, especially for interactions?

- Just because a point has high leverage does not necessarily mean it will have a substantial impact on the regression. Therefore you should check other measures.

---

### Standardized &amp; Studentized Residuals

- What is the best way to identify outliers (points that don't fit the pattern from the regression line)? 
  
--

- Look for points that have large residuals

--

- We want a common scale, so we can more easily identify "large" residuals

--

- We will look at each residual divided by its standard error

---

### Standardized Residuals 

.alert[
`$$std.res_i = \frac{e_i}{\hat{\sigma}\sqrt{1-h_i}}$$`
]
&lt;br&gt;

--

- The standard error of a residual, `\(\hat{\sigma}\sqrt{1-h_i}\)` depends on the value of the predictor variables 

--


- Residuals for observations that are high leverage have smaller variance than residuals for observations that are low leverage 

  + This is because the regression line tries to fit high leverage observations as closely as possible
  
---

### Standardized Residuals 

- Values with very large standardized residuals are outliers, since they don't fit the pattern determined by the regression model 

- Observations with standardized residuals of magnitude `\(&gt; 2\)` should be examined more closely

- Observations with large standardized residuals are outliers but may not have an impact on the regression line

- &lt;font class="vocab"&gt;Good Practice: &lt;/font&gt;Make residual plots with standardized residuals
    - It is easier to identify outliers and check for constant variance assumption

---

### Motivating Cook's Distance

- If a observation has a large impact on the estimated regression coefficients, when we drop that observation...
  + The estimated coefficients should change 
  + The predicted `\(Y\)` value for that observation should change
  
- One way to determine each observation's impact could be to delete it, rerun the regression, compare the predicted `\(Y\)` values from the new and original models
  + This could be very time consuming 
  
- Instead, we can use &lt;font class="vocab3"&gt;Cook's Distance&lt;/font&gt; which gives a measure of the change in the predicted `\(Y\)` value when an observation is dropped

---

### Cook's Distance

- &lt;font class="vocab3"&gt;Cook's Distance: &lt;/font&gt; Measure of an observation's overall impact, i.e. the effect removing the observation has on the estimated coefficients

- For the `\(i^{th}\)` observation, we can calculate Cook's Distance as 
.alert[
`$$D_i = \frac{1}{p}(std.res_i)^2\bigg(\frac{h_i}{1-h_i}\bigg)$$`
]

- *Note:* Cook's distance, `\(D_i\)`, incorporates both the residual and the leverage for each observation

- An observation with large `\(D_i\)` is said to have a strong influence on the predicted values 

---

### Using these measures

- Standardized residuals, leverage, and Cook's Distance should all be examined together 

- Examine plots of the measures to identify observations that may have an impact on your regression model 

- Some thresholds for flagging potentially influential observations:
  + **Leverage**: `\(h_i &gt; \frac{2(p+1)}{n}\)` (some software uses `\(2p/n\)`)
  + **Standardized Residuals**: `\(|std.res_i| &gt; 2\)`
  + **Cook's Distance**: `\(D_i &gt; 1\)`

---

### What to do with outliers/influential observations?

- It is **&lt;font color="green"&gt;OK&lt;/font&gt;** to drop an observation based on the **&lt;u&gt;predictor variables&lt;/u&gt;** if...
    - It is meaningful to drop the observation given the context of the problem 
    - You intended to build a model on a smaller range of the predictor variables. Mention this in the write up of the results and be careful to avoid extrapolation when making predictions

--

- It is **&lt;font color="red"&gt;not OK&lt;/font&gt;** to drop an observation based on the response variable
    - These are legitimate observations and should be in the model
- You can try transformations or increasing the sample size by collecting more data

--

- In either instance, you can try building the model with and without the outliers/influential observations

---

### Model diagnostics in R

- Use the &lt;font class="vocab"&gt;`augment`&lt;/font&gt; function in the broom package to output the model diagnostics (along with the predicted values and residuals)

- Output from `augment` : 
    - response and predictor variables in the model
    - `.fitted`: predicted values
    - `.se.fit`: standard errors of predicted values
    - `.resid`: residuals
    - **`.hat`**: leverage
    - `.sigma`: estimate of residual standard deviation when corresponding observation is dropped from model
    - **`.cooksd`**: Cook's distance
    - **`.std.resid`**: standardized residuals

---

### Example: Restaurant tips

What affects the amount customers tip at a restaurant?

- **Response:**
    - &lt;font class="vocab"&gt;`Tip`&lt;/font&gt;: amount of the tip
    
- **Predictors:**
    - &lt;font class="vocab"&gt;`Party`&lt;/font&gt;: number of people in the party
    - &lt;font class="vocab"&gt;`Meal`&lt;/font&gt;:  time of day (Lunch, Dinner, Late Night) 
    - &lt;font class="vocab"&gt;`Age`&lt;/font&gt;: age category of person paying the bill (Yadult, Middle, SenCit)


```r
tips &lt;- read_csv("data/tip-data.csv") %&gt;%
  filter(!is.na(Party))
```


---

### Example: Tips


```r
model1 &lt;- lm(Tip ~ Party + Meal + Age , data = tips)
kable(tidy(model1),format="html",digits=3)
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.254 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.394 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.182 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.002 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Party &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.808 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.121 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 14.909 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; MealLate Night &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.632 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.407 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -4.013 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; MealLunch &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.612 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.402 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.523 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.130 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; AgeSenCit &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.390 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.394 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.990 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.324 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; AgeYadult &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.505 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.412 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.227 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.222 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

### Using `augment` function

- Use the `augment` function to add predicted values and model diagnostics to data
    - Add the observation number for diagnostic plots
    

```r
tips_output &lt;- augment(model1) %&gt;%
  mutate(obs_num = row_number())
```

---

### Augmented data


```r
glimpse(tips_output)
```

```
## Observations: 169
## Variables: 12
## $ Tip        &lt;dbl&gt; 2.99, 2.00, 5.00, 4.00, 10.34, 4.85, 5.00, 4.00, 5.00…
## $ Party      &lt;dbl&gt; 1, 1, 1, 3, 2, 2, 4, 3, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2,…
## $ Meal       &lt;chr&gt; "Dinner", "Dinner", "Dinner", "Dinner", "Dinner", "Di…
## $ Age        &lt;chr&gt; "Yadult", "Yadult", "SenCit", "Middle", "SenCit", "Mi…
## $ .fitted    &lt;dbl&gt; 2.5562830, 2.5562830, 3.4515838, 6.6766419, 5.2591209…
## $ .se.fit    &lt;dbl&gt; 0.3771863, 0.3771863, 0.3939434, 0.2327069, 0.3604347…
## $ .resid     &lt;dbl&gt; 0.43371698, -0.55628302, 1.54841620, -2.67664190, 5.0…
## $ .hat       &lt;dbl&gt; 0.03722423, 0.03722423, 0.04060519, 0.01416878, 0.033…
## $ .sigma     &lt;dbl&gt; 1.960700, 1.960502, 1.957071, 1.949536, 1.918486, 1.9…
## $ .cooksd    &lt;dbl&gt; 3.294208e-04, 5.419132e-04, 4.612379e-03, 4.554814e-0…
## $ .std.resid &lt;dbl&gt; 0.226100143, -0.289994804, 0.808622859, -1.378941928,…
## $ obs_num    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…
```

---

### Leverage


```r
leverage_threshold &lt;- 2*(5+1)/nrow(tips)
```


```r
ggplot(data = tips_output, aes(x = obs_num,y = .hat)) + 
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept = leverage_threshold,color = "red")+
  labs(x = "Observation Number",y = "Leverage",title = "Leverage") +
  geom_text(aes(label=ifelse(.hat &gt; leverage_threshold, as.character(obs_num), "")), nudge_x = 4)
```

&lt;img src="11-model-diagnostics_files/figure-html/unnamed-chunk-18-1.png" style="display: block; margin: auto;" /&gt;

---

### Points with high leverage


```r
tips_output %&gt;% filter(.hat &gt; leverage_threshold) %&gt;% 
  select(Party, Meal, Age)
```

```
## # A tibble: 5 x 3
##   Party Meal       Age   
##   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; 
## 1     8 Late Night Middle
## 2     7 Dinner     SenCit
## 3     6 Dinner     SenCit
## 4     6 Late Night Middle
## 5     9 Lunch      SenCit
```

.question[
Why do you think these points have high leverage?
]

---

### Standardized Residuals vs. Predicted


```r
ggplot(data = tips_output, aes(x = .fitted,y = .std.resid)) +
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept = 0,color = "red") +
  geom_hline(yintercept = -2,color = "red",linetype = "dotted") +
  geom_hline(yintercept = 2,color = "red",linetype = "dotted") +
  labs(x ="Predicted Value",y ="Standardized Residuals",title = "Standardized Residuals vs. Predicted") +
  geom_text(aes(label = ifelse(abs(.std.resid) &gt;2,as.character(obs_num),"")), nudge_x = 0.3)
```

&lt;img src="11-model-diagnostics_files/figure-html/unnamed-chunk-20-1.png" style="display: block; margin: auto;" /&gt;

---

### Standardized residuals vs. predictors 

&lt;img src="11-model-diagnostics_files/figure-html/unnamed-chunk-21-1.png" style="display: block; margin: auto;" /&gt;

---

### Points with large magnitude std.res.


```r
tips_output %&gt;% filter(abs(.std.resid) &gt; 2) %&gt;% 
  select(Party, Meal, Age, Tip)
```

```
## # A tibble: 8 x 4
##   Party Meal       Age      Tip
##   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt;
## 1     2 Dinner     SenCit  10.3
## 2     2 Dinner     Yadult  11  
## 3     8 Late Night Middle  19.5
## 4     5 Lunch      Middle  15  
## 5     4 Dinner     Middle  16  
## 6     6 Late Night Middle   5  
## 7     4 Lunch      Middle   4  
## 8     3 Lunch      Middle  10
```

.question[
- Why do you think these points have standardized residuals with large magnitude?
- What other variables could you examine?
]

---

### Why we want to find outliers

Estimate of regression standard deviation, `\(\hat{\sigma}\)`, using all observations


```r
glance(model1)$sigma
```

```
## [1] 1.954983
```

Estimate of `\(\hat{\sigma}\)` without points with large magnitude standardized residuals


```r
tips_output %&gt;%
  filter(abs(.std.resid) &lt;= 2) %&gt;%
  summarise(sigma_est = sqrt(sum(.resid^2)/(n() - 5 - 1)))
```

```
## # A tibble: 1 x 1
##   sigma_est
##       &lt;dbl&gt;
## 1      1.56
```

---

class: middle 

**Recall that we use `\(\hat{\sigma}\)` to calculate the standard errors for all confidence intervals and p-values, so outliers can affect conclusions drawn from model**

---

### Cook's Distance


```r
ggplot(data = tips_output, aes(x = obs_num, y = .cooksd)) + 
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept=1,color = "red")+
  labs(x= "Observation Number",y = "Cook's Distance",title = "Cook's Distance") +
  geom_text(aes(label = ifelse(.hat&gt;1,as.character(obs_num),"")))
```

&lt;img src="11-model-diagnostics_files/figure-html/unnamed-chunk-25-1.png" style="display: block; margin: auto;" /&gt;

---

class: middle

See the supplemental notes [Details on Model Diagnostics](https://github.com/sta210-fa19/supplemental-notes/blob/master/model-diagnostics-matrix.pdf) for more details about standardized residuals, leverage points, and Cook's distance.


---

class: middle, center

## Multicollinearity

---

### Why multicollinearity is a problem

- We can't include two variables that have a perfect linear association with each other

- If we did so, we could not pick a unique best fit model


---

### Why multicollinearity is a problem

- Ex. Suppose the true population regression equation is `\(y = 3 + 4x\)`

--

- Suppose we try estimating that regression model using the variables `\(x\)` and `\(z = x/10\)`
`$$\begin{aligned}\hat{y}&amp;= \hat{\beta}_0 + \hat{\beta}_1x  + \hat{\beta}_2\frac{x}{10}\\
&amp;= \hat{\beta}_0 + \bigg(\hat{\beta}_1 + \frac{\hat{\beta}_2}{10}\bigg)x\end{aligned}$$`

--

- We can set `\(\hat{\beta}_1\)` and `\(\hat{\beta}_2\)` to any two numbers such that `\(\hat{\beta}_1 + \frac{\hat{\beta}_2}{10} = 4\)`
  + We are unable then to choose the "best" combination of `\(\hat{\beta}_1\)` and `\(\hat{\beta}_2\)`
  
---

### Why multicollinearity is a problem

- When we have almost perfect collinearities (i.e. highly correlated explanatory variables), the standard errors for our regression coefficients inflate

- In other words, we lose precision in our estimates of the regression coefficients 
  
---
 
### Detecting Multicollinearity

Multicollinearity may occur when...
- There are very high correlations `\((r &gt; 0.9)\)` among two or more explanatory variables, especially for smaller sample sizes

--

- One (or more) explanatory variables is an almost perfect linear combination of the others 

--

- Include quadratic terms without first mean-centering the variables before squaring

--

- Including interactions with two or more continuous variables

---

 

### Detecting Multicollinearity 

- Look at a correlation matrix of the predictor variables, including all indicator variables 
  + Look out for values close to 1 or -1

- If you think one predictor variable is an almost perfect linear combination of other predictor variables, you can run a regression of that predictor variable vs. the others and see if `\(R^2\)` is close to 1

---

### Detecting Multicollinearity (VIF)

- &lt;font class="vocab"&gt;Variance Inflation Factor (VIF)&lt;/font&gt;: Measure of multicollinearity 

.alert[
`$$VIF(\hat{\beta}_j) = \frac{1}{1-R^2_{X_j|X_{-j}}}$$`
]

where `\(R^2_{X_j|X_{-j}}\)` is the proportion of variation `\(X\)` that is explained by the linear combination of the other explanatory variables in the model.


- Typically `\(VIF &gt; 10\)` indicates concerning multicollinearity


- Use the &lt;font class="vocab"&gt;`vif()`&lt;/font&gt; function in the `rms` package to calculate VIF

---

### Tips VIF

- Calculate VIF using the &lt;font class="vocab"&gt;`vif`&lt;/font&gt; function in the rms package 


```r
library(rms)
tidy(vif(model1))
```

```
## # A tibble: 5 x 2
##   names              x
##   &lt;chr&gt;          &lt;dbl&gt;
## 1 Party           1.19
## 2 MealLate Night  1.25
## 3 MealLunch       1.09
## 4 AgeSenCit       1.10
## 5 AgeYadult       1.40
```

---

### Calculating VIF for `Party`


```r
party_model &lt;- lm(Party ~ Meal + Age, data=tips)
r.sq &lt;- glance(party_model)$r.squared
(vif &lt;- 1/(1-r.sq))
```

```
## [1] 1.193821
```

---

### Calculating VIF for `MealLateNight`


```r
# create indicator variables for Meal
tips &lt;- tips %&gt;% 
  mutate(late_night = if_else(Meal=="Late Night",1,0), 
         lunch = if_else(Meal=="Lunch",1,0))
```


```r
late_night_model &lt;- lm(late_night ~ lunch + Party + Age, data=tips)
r.sq &lt;- glance(late_night_model)$r.squared
(vif &lt;- 1/(1-r.sq))
```

```
## [1] 1.250908
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": "%current%"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
