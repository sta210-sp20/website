---
title: "Multinomial Logistic Regression"
subtitle: "Predictions & Drop-in Deviance Test"
author: "Dr. Maria Tackett"
date: "11.04.19"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta210-slides.css"
    logo: img/sta210-sticker-icon.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%"
---

```{r setup, include=FALSE, echo=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
	fig.align = "center",
	fig.height = 5.5,
	fig.width = 10,
	message = FALSE,
	warning = FALSE
)
```

class: middle, center

### [Click for PDF of slides](20-multinomial-logistic-pt2.pdf)
---

### Announcements

- Multinomial Logistic Regression: [Reading 10](https://www2.stat.duke.edu/courses/Fall19/sta210.001/reading/reading-10.html) and [Reading 11](https://www2.stat.duke.edu/courses/Fall19/sta210.001/reading/reading-11.html)

- HW 05 **due TODAY at 11:59p**

```{r, echo = F}
library(tidyverse)
library(knitr)
library(broom)
```

---

### Generalized Linear Models (GLM)

- In practice, there are many different types of response variables including:

  + **Binary**: Win or Lose
  + **Nominal**: Democrat, Republican or Third Party candidate
  + **Ordered**: Movie rating (1 - 5 stars)
  + and others...

- These are all examples of **generalized linear models**, a broader class of models that generalize the multiple linear regression model 

- See [*Generalized Linear Models: A Unifying Theory*](https://bookdown.org/roback/bookdown-bysh/ch-glms.html) for more details about GLMs

---

### Binary Response (Logistic)

- Suppose we consider $y=0$ the *<font color="blue">baseline category</font>* such that 

$$P(y_i=0|x_i) = p_{i0} \text{ and } P(y_i=1|x_i) = p_{i1}$$

--

- Then the logit model is 

$$\log\bigg(\frac{p_{i1}}{p_{i0}}\bigg) = \beta_0 + \beta_1 x_i$$

--

- <font class="vocab">Slope, $\beta_1$</font>: When $x$ increases by one unit, the odds of $Y=1$ versus the baseline $Y=0$ are expected to multiply by a factor of $\exp\{\beta_1\}$

- <font class="vocab3">Intercept, $\beta_0$</font>: When $x=0$, the odds of $y=1$ versus the baseline $y=0$ are expected to be $\exp\{\beta_0\}$

---

### Multinomial response variable

- Suppose the response variable $y$ is categorical and can take values $1, 2, \ldots, k$ such that $(k > 2)$

- <font class="vocab">Multinomial Distribution: </font>

$$P(Y=1) = p_1, P(Y=2) = p_2, \ldots, P(Y=k) = p_k$$

such that $\sum\limits_{j=1}^{k} p_j = 1$

---

### Multinomial Logistic Regression 

- Suppose we have a response variable $Y$ that can take three possible outcomes that are coded as "1", "2", "3"

- Let "1" be the baseline category. Then 

.alert[
$$\log\bigg(\frac{p_{i2}}{p_{i1}}\bigg) = \beta_{02} + \beta_{12} X_i \\[15pt]
\log\bigg(\frac{p_{i3}}{p_{i1}}\bigg) = \beta_{03} + \beta_{13} X_i$$
]

---

### Multinomial Regression in R 

- Use the <font class="vocab">`multinom()`</font> function in the `nnet` package 

```{r eval=F}
library(nnet)
my.model <- multinom(Y ~ X1 + X2 + ... + XP, data=my.data)
tidy(my.model, exponentiate = FALSE) #display log-odds model
```
<br> 

```{r eval=F}
# calculate predicted probabilities
pred.probs <- predict(my.model, type = "probs")
```

---

### NHANES Data

- [National Health and Nutrition Examination Survey](https://www.cdc.gov/nchs/nhanes/index.htm) is conducted by the National Center for Health Statistics (NCHS) 

- The goal is to *"assess the health and nutritional status of adults and children in the United States"*

- This survey includes an interview and a physical examination

---

### NHANES Data

- We will use the data from the <font class="vocab">`NHANES`</font> R package

- Contains 75 variables for the 2009 - 2010 and 2011 - 2012 sample years

- The data in this package is modified for educational purposes and should **not** be used for research

- Original data can be obtained from the [NCHS website](https://www.cdc.gov/nchs/data_access/index.htm) for research purposes

- Type <font class="vocab">`?NHANES`</font> in console to see list of variables and definitions

---

### NHANES: Health Rating vs. Age & Physical Activity

- **Question**: Can we use a person's age and whether they do regular physical activity to predict their self-reported health rating?

- We will analyze the following variables: 

  + <font class="vocab">`HealthGen`: </font>Self-reported rating of participant's health in general.  Excellent, Vgood, Good, Fair, or Poor.
  
    + <font class="vocab">`Age`: </font>Age at time of screening (in years). Participants 80 or older were recorded as 80.
    
  + <font class="vocab">`PhysActive`: </font>Participant does moderate to vigorous-intensity sports, fitness or recreational activities

 
---

### The data

```{r}
library(NHANES)

nhanes_adult <- NHANES %>%
  filter(Age >= 18) %>%
  select(HealthGen, Age, PhysActive, Education) %>%
  drop_na() %>%
  mutate(obs_num = 1:n())
```

```{r}
glimpse(nhanes_adult)
```

---

### Exploratory data analysis

```{r echo = F}
library(cowplot)
p1 <- ggplot(data = nhanes_adult, aes(x = Age)) + 
  geom_histogram() +
  labs(title = "Distribution of Age")

p2 <- ggplot(data = nhanes_adult, aes(x = PhysActive)) + 
  geom_bar() +
  labs(title = "Moderate or vigorous sport or exercise")

p3 <- ggplot(data = nhanes_adult, aes(x = HealthGen)) + 
  geom_bar() +
  labs(title = "Self-reported rating of overall health")

top_row <- plot_grid(p1, p2, ncol = 2)
plot_grid(top_row, p3, ncol = 1)
```

---

### Exploratory data analysis

```{r echo = F}
p1 <- ggplot(data = nhanes_adult, aes(x = HealthGen, y = Age)) +
  geom_boxplot(fill = "steelblue") + 
  labs(title = "Age vs. Health Rating")

p2 <- ggplot(data = nhanes_adult, aes(x = HealthGen, fill = PhysActive)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", 
       title = "Physical Activity vs. Health Rating") +
  coord_flip()

plot_grid(p1, p2, ncol = 2)
```

---

### `HealthGen` vs. `Age` and `PhysActive`

```{r, results = "hide"}
library(nnet)
health_m <- multinom(HealthGen ~ Age + PhysActive, 
                     data = nhanes_adult)
```

- Put `results = "hide"` in the code chunk header to suppress convergence output 

---

### `HealthGen` vs. `Age` and `PhysActive`

```{r}
tidy(health_m, exponentiate = FALSE, conf.int = TRUE) %>%
  kable(digits = 3, format = "markdown")
```

---

### Interpreting coefficients

.question[
1. What is the model baseline category, i.e. the baseline category of the response variable? 

2. Write the model for the odds that a person rates themselves as having "Fair" health versus the model baseline category.

3. Interpret the coefficient for `Age` in terms of the odds that a person rates themselves has having "Poor" heath versus the model's baseline category
]

---

### Model assessment

For each category of the response, $j$:

- Analyze a plot of the binned residuals vs. predicted probabilities 

- Analyze a plot of the binned residuals vs. each continuous predictor variable

- Look for any patterns in the residuals plots

- For each categorical predictor variable, examine the average residuals for each category of the response variable


---

### NHANES: Predicted probabilities 
 
```{r}
#calculate predicted probabilities
pred_probs <- as_tibble(predict(health_m, type = "probs")) %>% 
                        mutate(obs_num = 1:n()) 
```

```{r}
pred_probs %>%
  slice(1:10)
```

---

### NHANES: Residuals

```{r}
#calculate residuals
residuals <- as_tibble(residuals(health_m)) %>%  #calculate residuals
  setNames(paste('resid.', names(.), sep = "")) %>% #update column names
  mutate(obs_num = 1:n()) #add obs number
```

```{r}
residuals %>%
  slice(1:10)
```

---

### Make "augmented" dataset

```{r}
health_m_aug <- inner_join(nhanes_adult, pred_probs) #add probs
health_m_aug <- inner_join(health_m_aug, residuals) #add resid
```

```{r}
health_m_aug %>% 
  glimpse()
```

---

### Binned residuals vs. pred. probabilities

```{r echo = F, fig.height = 4.5}
par(mfrow = c(2,2))
arm::binnedplot(x = health_m_aug$Excellent, y = health_m_aug$resid.Excellent, 
                xlab="Predicted P(Excellent)", 
                ylab="Residuals", 
                main="Excellent: Residuals vs. Pred. Probabilities", 
                col.int = FALSE)

arm::binnedplot(x = health_m_aug$Vgood, y = health_m_aug$resid.Vgood, 
                xlab="Predicted P(Vgood)", 
                ylab="Residuals", 
                main="Very Good: Residuals vs. Pred. Probabilities",
                col.int = FALSE)

arm::binnedplot(x = health_m_aug$Good, y = health_m_aug$resid.Good, 
                xlab="Predicted P(Good)", 
                ylab="Residuals", 
                main="Good: Residuals vs. Pred. Probabilities",
                col.int = FALSE)


arm::binnedplot(x = health_m_aug$Fair, y = health_m_aug$resid.Fair, 
                xlab="Predicted P(Fair)", 
                ylab="Residuals", 
                main="Fair: Residuals vs. Pred. Probabilities",
                col.int = FALSE)

arm::binnedplot(x = health_m_aug$Poor, y = health_m_aug$resid.Poor, 
                xlab="Predicted P(Poor)", 
                ylab="Residuals", 
                main="Poor: Residuals vs. Pred. Probabilities",
                col.int = FALSE)
```

---

### Binned residuals vs. `Age`  

```{r echo = F, fig.height = 4.5}
par(mfrow = c(2,2))
arm::binnedplot(x = health_m_aug$Age, y = health_m_aug$resid.Excellent, 
                xlab="Age", 
                ylab="Residuals", 
                main="Excellent: Residuals vs. Age", 
                col.int = FALSE)

arm::binnedplot(x = health_m_aug$Age, y = health_m_aug$resid.Vgood, 
                xlab="Age", 
                ylab="Residuals", 
                main="Very Good: Residuals vs. Age",
                col.int = FALSE)


arm::binnedplot(x = health_m_aug$Age, y = health_m_aug$resid.Good, 
                xlab="Age", 
                ylab="Residuals", 
                main="Good: Residuals vs. Age",
                col.int = FALSE)


arm::binnedplot(x = health_m_aug$Age, y = health_m_aug$resid.Fair, 
                xlab="Age", 
                ylab="Residuals", 
                main="Fair: Residuals vs. Age",
                col.int = FALSE)

arm::binnedplot(x = health_m_aug$Age, y = health_m_aug$resid.Poor, 
                xlab="Age", 
                ylab="Residuals", 
                main="Poor: Residuals vs. Age",
                col.int = FALSE)
```

---
### Residuals vs. `PhysActive`

```{r}
health_m_aug %>%
  group_by(PhysActive) %>%
  summarise(mean.Excellent = mean(resid.Excellent), 
            mean.Vgood = mean(resid.Vgood), 
            mean.Good = mean(resid.Good),
            mean.Fair = mean(resid.Fair), 
            mean.Poor = mean(resid.Poor)) %>%
  t()
```

---

## Calculating probabilities

For $j = 2,\ldots, k$, we calculate the probability $p_{ij}$ as 

.alert[
$$p_{ij} = \frac{\exp\{\beta_{0j} + \beta_{1j}x_i\}}{1 + \sum\limits_{j=2}^k \exp\{\beta_{0j} + \beta_{1j}x_i\}}$$
]

--

For the baseline category $(j = 1)$ we calculate the probability $(p_{i1})$ as
.alert[
$$p_{i1} = 1- \sum\limits_{j=2}^k p_{ij}$$
]

--

We will use these probabilities to assign a category of the response for each observation 

---

### Actual vs. Predicted Health Rating

- We can use our model to predict a person's health rating given their age and whether they exercise

- For each observation, the predicted health rating is the one with the highest predicted probability

```{r}
health_m_aug <- 
  health_m_aug %>% 
  mutate(pred_health = predict(health_m, type = "class"))
```

---

### Actual vs. Predicted Health Rating

```{r}
health_m_aug %>% 
  count(HealthGen, pred_health, .drop = FALSE) %>%
  pivot_wider(names_from = pred_health, values_from = n)
#rows = actual, columns = predicted
```

---

## Predictions 

```{r echo = F}
health_m_aug %>%
  select(Excellent, Vgood, Good, Fair, Poor, pred_health) %>%
  slice(1:5)
```

---

### Drop-in-deviance Test

- Suppose there are two models: 
    - Model 1 includes predictors $x_1, \ldots, x_q$
    - Model 2 includes predictors $x_1, \ldots, x_q, x_{q+1}, \ldots, x_p$

- We want to test the hypotheses
$$\begin{aligned}&H_0: \beta_{q+1} = \dots = \beta_p = 0 \\
& H_a: \text{ at least 1 }\beta_j \text{ is not} 0\end{aligned}$$

- Use the **drop-in-deviance test** to compare models (similar to logistic regression)

---

### Add `Education` to the model? 

- We consider adding the participants' `Education` level to the model.
    - Education takes values `8thGrade`, `9-11thGrade`, `HighSchool`, `SomeCollege`, and `CollegeGrad`

- Models we're testing: 
    - Model 1: `Age`, `PhysActive`
    - Model 2: `Age`, `PhysActive`, `Education`
    
.alert[
$$\begin{align}&H_0: \beta_{9-11thGrade} = \beta_{HighSchool} = \beta_{SomeCollege} = \beta_{CollegeGrad}\\
&H_a: \text{ at least one }\beta_j \text{ is not equal to }0\end{align}$$
]

---

### Add `Education` to the model? 

.alert[
$$\begin{align}&H_0: \beta_{9-11thGrade} = \beta_{HighSchool} = \beta_{SomeCollege} = \beta_{CollegeGrad}\\
&H_a: \text{ at least one }\beta_j \text{ is not equal to }0\end{align}$$
]

```{r results = "hide"}
m1 <- multinom(HealthGen ~ Age + PhysActive, 
               data = nhanes_adult)
m2 <- multinom(HealthGen ~ Age + PhysActive + Education, 
               data = nhanes_adult)
```

---

### Add `Education` to the model? 

```{r results = "hide"}
m1 <- multinom(HealthGen ~ Age + PhysActive, 
               data = nhanes_adult)
m2 <- multinom(HealthGen ~ Age + PhysActive + Education, 
               data = nhanes_adult)
```

```{r}
kable(anova(m1, m2, test = "Chisq"), format = "markdown")
```

--

At least one coefficient associated with `Education` is non-zero. Therefore, `Education` is a statistically significant predictor for `HealthGen`. 



