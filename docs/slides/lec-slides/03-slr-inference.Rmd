---
title: "Inference for SLR"
author: "Dr. Maria Tackett"
date: "01.23.19"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta210-slides.css"
    logo: img/sta210-sticker-icon.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
library(knitr)
opts_chunk$set(fig.height = 2.65, dpi = 300) 
# ggplot2 color palette with gray
color_palette <- list(gray = "#999999", 
                      salmon = "#E69F00", 
                      lightblue = "#56B4E9", 
                      green = "#009E73", 
                      yellow = "#F0E442", 
                      darkblue = "#0072B2", 
                      red = "#D55E00", 
                      purple = "#CC79A7")

options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  dpi = 300,
	fig.align = "center",
	fig.height = 3,
	fig.width = 5,
	message = FALSE,
	warning = FALSE
)
```

## Announcements 

- Lab 01 due **today at 11:59p**

- HW 01 due Wed, Jan 30 at 11:59p


---

## Check in

- Any questions on material from last time?

- Any questions on the lab?

- Any questions on workflow / course structure?

- **KNIT AND COMMIT**

---

## Agenda

- Recap: Simple linear regression basics

- Estimating $\sigma^2$

- $R^2$

- Inference for the slope, $\beta_1$

---

## Packages and Data


```{r load-packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(modelr)
library(fivethirtyeight)
```

```{r data}
movie_scores <- fandango %>%
  rename(tomatometer = rottentomatoes, audience = rottentomatoes_user)
```

---

## rottentomatoes.com

Can ratings from movie critics be used to predict the ratings from movie goers?

--

```{r, echo=FALSE,out.width = '70%',fig.align='center'}
knitr::include_graphics("img/03/movie-rating-2.png")
```

--

```{r, echo=FALSE,out.width = '70%',fig.align='center'}
knitr::include_graphics("img/03/movie-rating-1.png")
```

---

## Critic vs. Audience Ratings

- To answer this question, we will analyze the critic and audience scores from rottentomatoes.com.  
    - The data was first used in the article [Be Suspicious of Online Movie Ratings, Especially Fandango's](https://fivethirtyeight.com/features/fandango-movies-ratings/).

- Variables: 
    - `tomatometer`: Rotten Tomatoes Tomatometer score for the film (0 - 100)
    - `audience`: Rotten Tomatoes user score for the film (0 - 100)

---

class: middle, center

## Recap: SLR Basics

---

## Regression Model 

$$Y|X \sim N(\beta_0 + \beta_1 X,\sigma^2)$$

```{r, echo=FALSE,out.width = '80%',fig.align='center'}
knitr::include_graphics("img/03/regression.png")
```

- $\sigma$: the standard deviation of $Y$ as a function of $X$ 

- **Assumption:** $\sigma$ is equal for all values of $X$

---

## Regression Model

.alert[
$$\color{blue}{Y|X \sim N(\beta_0 + \beta_1 X,\sigma^2)}$$
]

--

- For a single observation $(x_i, y_i)$

$$\color{blue}{y_i = \beta_0 + \beta_1 x_i + \epsilon_i \hspace{10mm} \epsilon_i \sim N(0,\sigma^2)}$$

--

- We want to use the $n$ observations $(x_1,y_1), \ldots, (x_n, y_n)$ to estimate $\beta_0$ and $\beta_1$. We will use *least-squares regression* estimates.

---

## Scatterplot 

```{r message=FALSE, warning=FALSE, echo=FALSE}
ggplot(data=movie_scores,mapping=aes(x=tomatometer,y=audience)) + 
  geom_point() + 
  geom_smooth(method="lm",se=FALSE) +
  labs(title="Audience Score vs. Tomatometer") + 
  theme_gray()
```

---

## Simple Linear Regression Model

```{r slr-model}
model <- lm(audience ~ tomatometer, data=movie_scores)
tidy(model)
```

- **Slope**: For every one percentage increase in the Tomatometer score, we expect the audience score to increase by `r round(model$coefficients[2],3)`%. 

- **Intercept**: If the Tomatometer score is 0%, we expect the audience score to be about `r round(model$coefficients[1],3)`%.

---

## Assumptions for Regression 

1. <font class="vocab">Linearity: </font>The plot of the mean value for $y$ against $x$ falls on a straight line

2. <font class="vocab">Constant Variance: </font>The regression variance is the same for all values of $x$

3. <font class="vocab">Normality: </font> For a given $x$, the distribution of $y$ around its mean is Normal

4. <font class="vocab">Independence: </font>All observations are independent

---

## Linearity & Constant variance

```{r calc-residuals, echo=FALSE}
movie_scores <- movie_scores %>% mutate(residuals=resid(model))
```

```{r echo=FALSE, fig.width=4, fig.height=2}
ggplot(data=movie_scores,mapping=aes(x=tomatometer, y=residuals)) + 
  geom_point() + 
  geom_hline(yintercept=0,color="red")+
  labs(title="Residuals vs. Tomatometer") +
  theme_gray()
```

- From the scatterplot and plot of the residuals vs. predictor, we conclude the **linearity** assumption is sufficiently met.

- From the plot of the residuals vs. predictor, we conclude the **constant variance** assumption is sufficiently met. 

---

## Normality of residuals 
```{r echo=FALSE}
library(cowplot)

p1 <- ggplot(data=movie_scores,mapping=aes(x=residuals)) + 
  geom_histogram() + 
  labs(title="Distribution of Residuals") +
  theme_gray()

p2 <- ggplot(data=movie_scores,mapping=aes(sample=residuals)) + 
  stat_qq() + 
  stat_qq_line() +
  labs(title="Normal QQ Plot of Residuals") +
  theme_gray()

plot_grid(p1,p2,ncol=2)
```

- From the histogram and QQ-plot, we conclude that the **Normality** assumption is met.

---

## Independence

- If we assume critics and audiences judge each movie independently, then we could conclude that the independence assumption is reasonably met. 

- There potentially could be a cluster effect due to movie genre, so a good next step would be to analyze the residuals by genre. 

--

- Due to time constraints, let's assume the independence assumption is reasonably met for the class notes.

---

class: middle, center

## Assessing Model Fit

---

class: regular 

## Estimating Regression Variance

- <font class="vocab3">Residual: </font>Difference between the observed response and its estimated mean (the predicted response)
  + The residual for the $i^{th}$ observation, $(X_i, Y_i)$, is $$e_i = Y_i - (\hat{\beta}_0+ \hat{\beta}_1 X_i)$$
--

- <font class="vocab3">Residual Standard Error (RSE)</font>: Approximately the average amount the response will deviate from the true regression line. 
    - It is the estimate of $\sigma$

$$\hat{\sigma} = \sqrt{\frac{1}{n-2}RSS} \hspace{2.5mm} = \hspace{2.5mm} \sqrt{\frac{1}{n-2}\sum\limits_{i=1}^n (y_i - \hat{y}_i)^2}$$

---

## Calculating RSE in R

```{r}
model %>% sigma()
```
<br>
<br>

One way we can use the RSE is to compare the fit of two models. **What is one disadvantage of using the RSE for this purpose?**

---

## $R^2$

We can use the coefficient of determination, $R^2$, to measure how well the model fits the data 

- $R^2$ is the proportion of variation in $Y$ that is explained by the regression line (reported as percentage)

- It is difficult to determine what exactly is a "good" value of $R^2$. It depends on the context of the data.

---

## Calculating $R^2$

.instructions[
$$R^2 = \frac{\text{TSS} - \text{RSS}}{\text{TSS}} = 1 - \frac{\text{RSS}}{\text{TSS}}$$
]

- <font class="vocab">Total Sum of Squares: </font>Total variation in the $Y$'s before fitting the regression 

$$\text{TSS}= \sum\limits_{i=1}^{n}(y_i - \bar{y})^2 = (n-1)s^2_y$$

- <font class="vocab">Residual Sum of Squares (RSS): </font>Total variation in the $Y$'s around the regression line (sum of squared residuals)
$$\text{RSS} = \sum\limits_{i=1}^{n}[y_i - (\hat{\beta}_0 + \hat{\beta}_1x_i)]^2$$

---

## Rotten Tomatoes Data

```{r calc-rsquare}
rsquare(model,movie_scores)
```

The Tomatometer score explains about `r round(rsquare(model,movie_scores) * 100,2)`% of the variation in audience scores on rottentomatoes.com.

---

class: middle, center

## Inference for $\beta_1$

---

## Questions of interest

In our example, we will treat the data as a random sample of movies from rottentomatoes.com

**Questions of interest**
- What is a plausible range of values of the true population slope for `tomatometer`?
- Is there truly a linear relationship between the tomatometer and audience scores? In other words, is there enough evidence to conclude that the true population slope is different from 0?

---

## Statistical Inference

- <font class="vocab">Confidence Interval: </font> Estimate a range of plausible values for a parameter

- <font class="vocab">Hypothesis Test: </font> Test a specified claim or hypothesis about the parameter

- In this class, our focus will be on inference for regression coefficients, i.e. the $\beta_j$'s

---


## Questions of interest

- What is a plausible range of values of the true population slope for `tomatometer`? (<font class="vocab">Confidence Interval</font>)

- Is there truly a linear relationship between the tomatometer and audience scores? In other words, is there enough evidence to conclude that the true population slope is different from 0? (<font class="vocab">Hypothesis Test</font>)

---

class: middle, center

**What is a plausible range of values of the true population slope for `tomatometer`?**

---

## Confidence Intervals

- **What**: Estimates a population parameter using a sample statistic
    - Assuming sample data is a simple random sample
  
- **Why**: Because the statistic is a random variable, its value is subject to chance error
    - We want a range of plausible values for the population parameter that takes the chance error into account

- We assume the data is from a random sample that is representative of the population. A confidence interval will not make up for systematic bias in the data


---

### Understanding 95% Confidence Interval

.center[
```{r, echo=FALSE,out.width = '40%'}
knitr::include_graphics("img/03/confidence_intervals.png")
```
]
- It's a procedure to produce an interval for the parameter of interest
- If we take many samples of size $n$, the intervals calculated from the sample will contain the parameter about 95% of the time
- It is **not** interpreted as the probability the parameter of interest is in a given interval

---

## General form of the CI

- Let <font class="vocab">SE</font> be the standard error of the statistic used to estimate the parameter of interest, then the general form of the confidence interval is

.alert[
$$\text{ Estimate} \pm \text{ (critical value) } \times \text{SE}$$
]
- *Note*: The critical value is determined by the distribution of the estimate (statistic) and the confidence level

- For the regression slope: 
    - $\hat{\beta}_1$ is the statistic used to estimate the parameter, $\beta_1$ 
    - We will write the confidence interval as 
    $$\mathbf{\hat{\beta}_1 \pm t^* SE(\hat{\beta}_1)}$$
    
---

## Confidence interval for $\beta_1$

- The confidence interval for the regression slope is 

.alert[
$$\mathbf{\hat{\beta}_1 \pm t^* SE(\hat{\beta}_1)}$$
]

- $t^*$ is the critical value associated with the confidence level.
  + It is calculated from a $t$ distribution with $n-2$ degrees of freedom
  
- $SE(\hat{\beta}_1)$ is the standard error for the slope 

$$SE(\hat{\beta}_1) = \sqrt{\frac{\hat{\sigma}^2}{\sum\limits_{i=1}^n (x_i - \bar{x})^2}} \hspace{2.5mm} = \hspace{2.5mm} \hat{\sigma}\sqrt{\frac{1}{(n-1)s_X^2}}$$

---

## t-distribution vs. Normal 

- Since we calculate $SE(\hat{\beta}_1)$ using $\hat{\sigma}$, an estimate of $\sigma$, we will use the *t* distribution to find the critical value for the confidence interval.

- The critical value $t^*$ is calculated from the *t(n-2)* distribution - the *t* distribution with *n-2* degrees of freedom.

```{r, echo=FALSE,out.width = '50%'}
knitr::include_graphics("img/03/tdistribution.png")
```

<font size="2">Picture from <i>The Basic Practice of Statistics (7th edition)</i></font>

---

### rottentomatoes: 95% confidence interval for $\beta_1$

```{r}
model %>%
  tidy(conf.int=TRUE)
```

**Interpretation**: 
  
We are 95% confident that the interval (0.450, 0.587) contains $\beta_1$, the true population slope for `tomatometer`. This means we are 95% confident that for every 1% increase in the tomatometer score, the audience score will increase between 0.45% and 0.587%, on average.

---

## Calculating the 95% CI

```{r}
sigma <- sigma(model)
beta1 <- nth(model$coefficients,2)
```


```{r}
crit.val <- qt(0.975,145)
```

```{r}
movie_scores %>%
  summarise(n = n(), 
            var=var(tomatometer),
            sigma = sigma,
            beta1 = beta1,
            crit.val = crit.val,
            se = sqrt(sigma^2 /((n-1)*var)), 
            lb = beta1 - crit.val * se,
            ub = beta1 + crit.val * se)
```


---

class: middle, center

**Is there truly a linear relationship between the tomatometer and audience scores?** 

**In other words, is there enough evidence to conclude that the true population slope is different from 0?**

---

## Hypthesis Tests

Question being answered: 

- Given the data in our sample, is there evidence <u>**against**</u> a specified hypothesis about the parameter of interest?

- In other words:
    - Are the data consistent with the specified hypothesis?
  
---

## Outline of a Hypothesis Test

- Assume some hypothesis about the parameter is true
    - This hypothesis is the <font class="vocab3">null hypothesis</font>

- Identify the appropriate statistic based on the distribution of the parameter of interest.
    - This statistic is the <font class="vocab3">test statistic.</font>
    - The statistic should take an extreme value when the hypothesis is false.

- Use the data in your sample to calculate the value of the test statistic.

---

## Outline of a Hypothesis Test

- Calculate the probability of observing a value of the statistic that is as extreme or more extreme than the observed value, under the assumed hypothesis.
    - This calculated probability is the <font class="vocab3">p-value</font>.

- Assess the p-value to make your conclusion. 

---

## Conducting a Hypothesis Test 

1. State the hypotheses

2. Calculate the test statistic 

3. Calculate the p-value

4. State the conclusion in the context of the problem

---

### 1. State the hypotheses

- We are often interested in testing whether there is a significant linear relationship between the explanatory and response variable 

- If there is no linear relationship between the two variables, the population regression slope should equal 0 
--

- We can test the hypotheses: 

$$\begin{aligned}&\mathbf{H_0: \boldsymbol{\beta_1} = 0}\\&\mathbf{H_a: \boldsymbol{\beta_1} \neq 0}\end{aligned}$$

- This is the test conducted by the `lm()` function in R


---

### 2. Calculate the test statistic

$$\begin{aligned}&H_0: \boldsymbol{\beta_1} = 0\\&H_a: \boldsymbol{\beta_1} \neq 0\end{aligned}$$

- <font class="vocab">Test Statistic: </font> 
$$\begin{aligned}\text{test statistic} &= \frac{\text{Estimate} - \text{Hypothesized}}{SE} \\
&= \frac{\hat{\boldsymbol{\beta}}_1 - 0}{SE(\hat{\beta}_1)}\end{aligned}$$

---

### 3. Calculate the p-value

- <font class="vocab">p-value:</font> Calculated from a $t$ distribution with $n-2$ degrees of freedom

$$\text{p-value} = P(t \geq |\text{test statistic}|)$$

A small p-value means either....

1. The assumed hypothesis is incorrect or 
2. The assumed hypothesis is correct and a rare event has occurred

---

### 4. State the conclusion 


- State a conclusion about the hypothesis based on the assessment of the p-value.
    - Since event (2) is by definition rare, we will conclude a <font color="green">small p-value</font> indicates that there <font color="green">is sufficient evidence</font> to claim <font color="green">that the assumed hypothesis is false</font>.
    
    - When the p-value is <font color="red">not small</font>, we will conclude that there <font color="red">is not sufficient evidence</font> to claim the assumed hypothesis is false.

---

### Hypothesis test for coefficient of `tomatometer`

```{r}
model %>%
  tidy(conf.int=TRUE)
```

---

class: middle, center

### Understanding Hypothesis Tests

---

### Can you define "p-value"?

- [Scientists Define *p*-value](https://fivethirtyeight.abcnews.go.com/video/embed/56150342)

- [Statisticians Found One Thing They Can Agree On: It's Time to Stop Misusing P-Values](https://fivethirtyeight.com/features/statisticians-found-one-thing-they-can-agree-on-its-time-to-stop-misusing-p-values/)

---

### What the *p-value* is and is not

**What the p-value is NOT**:
- It is <u><b>not</b></u> the probability the null hypothesis is true
  + The null hypothesis is either true or not true
- (1 - *p-value*) is <u><b>not</b></u> the probability that the alternative hypothesis is true
  + The alternative hypothesis is either true or not true
  
.alert[
**What the p-value <u>IS</u>**

The probability of getting a test statistic as extreme or more extreme than the calculated test statistic, *assuming the null hypothesis is true*
]

---

## Interpreting the p-value

|  Magnitude of p-value |             Interpretation            |
|:---------------------:|:-------------------------------------:|
| p-value < 0.01        | strong evidence against $H_0$         |
| 0.01 < p-value < 0.05 | moderate evidence against $H_0$       |
| 0.05 < p-value < 0.1  | weak evidence against $H_0$           |
| p-value > 0.1         | effectively no evidence against $H_0$ |
<br> 
<br>

**Note:** These are general guidelines. The strength of evidence depends on the context of the problem.

---

## Statistical Significance

- A threshold can be used to decide whether or not to reject $H_0$. 

- This threshold is called the <font class="vocab3">significance level</font> and is usually denoted by $\alpha$

- When $H_0$ is rejected, we use the term <font class="vocab3"> statistically significant </font> to describe the outcome of the test.

- *Example*: When $\alpha = 0.05$, results are statistically significance when the p-value is $< 0.05$

---

## Statistical Significance

- Do not rely strictly on the significance level to make a conclusion!
--

- Suppose the significance level is 0.05
--

  + If the p-value is 0.05001, we do not reject $H_0$
--

  + If the p-value is 0.04999, we do reject $H_0$
--

- 0.05001 and 0.04999 are practically the same, yet they led to different conclusions. 
--

- Always state the p-value when reporting results and assess their magnitude in the context of your problem. 

---

### Not Statistically Significant

- An outcome of failing to reject $H_0$ is <u>**not**</u> a failed study/experiment

- Obtaining an outcome of "no significant effect" or "no significant difference" is still valid 

- It is often just as important to learn that the $H_0$ can't be refuted

---

## Type I &  Type II Errors

```{r, echo=FALSE, fig.align='center', out.width = '80%'}
knitr::include_graphics("img/03/errors.png")
```
.small[
Image: *The Basic Practice of Statistics (7th Ed.)*
]

- <font class="vocab3">Type I Error</font>: Reject $H_0$ when $H_0$ is true
- <font class="vocab3">Type II Error</font>: Fail to reject $H_0$ when $H_1$ is true
- Replicate study when possible to reduce these errors

---

## Sample Size

- Probability of Type I error is not affected by the sample size
    - What affects the probability of Type I error?
--

- Probability of Type II error decreases as the sample size increases
--

- If the hypothesized value is not very different from actual parameter value, you need a large sample size
--

- When designing a study, it is good practice to conduct a power analyses to determine the sample size required to minimize the chance of Type II error

---

class: regular 

## Sample Size

- It is always preferable to collect as much data as possible (as long as it is accurate and relevant)
--

- A test can reject almost any false $H_0$, i.e. detect even very small effects, when the sample size is large enough
--

- *Note*: Statistical significance is <u>not</u> the same as practical significance 
  + Even if $H_0$ is rejected, the detected effect may be too small to be of any practical use

---

## Before next class 

- Lab 01 due **today at 11:59p**

- HW 1 due **Wed, Jan 30 at 11:59p**
   