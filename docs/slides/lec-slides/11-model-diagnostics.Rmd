---
title: "Multiple Linear Regression"
subtitle: "Model Diagnostics"
author: "Dr. Maria Tackett"
date: "10.02.19"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta210-slides.css"
    logo: img/sta210-sticker-icon.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%"
---

```{r setup, include=FALSE, echo=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
	fig.align = "center",
	fig.height = 3,
	message = FALSE,
	warning = FALSE, 
	dpi = 300
)
```

class: middle, center

### [Click for PDF of slides](11-model-diagnostics.pdf)

---

### Announcements

- HW 03 **due TODAY at 11:59p**

- HW 04 **due Thursday, October 10 at 11:59p**

- Thursday's lab: Help Hours

- Looking ahead: 
    - **Exam 1 on Mon, Oct 14 in class**
    - Practice exam on Sakai 
    - Can bring 1 page of notes
    - Exam review on Oct 9
    - Lecture notes, past assignments, and textbook to study
---

## R packages

```{r}
library(tidyverse)
library(knitr)
library(broom)
library(cowplot) # use plot_grid function
```

---

class: middle, center

## Nested F Test

---

## Restaurant tips

What affects the amount customers tip at a restaurant?

- **Response:**
    - <font class="vocab">`Tip`</font>: amount of the tip
    
- **Predictors:**
    - <font class="vocab">`Party`</font>: number of people in the party
    - <font class="vocab">`Meal`</font>:  time of day (Lunch, Dinner, Late Night) 
    - <font class="vocab">`Age`</font>: age category of person paying the bill (Yadult, Middle, SenCit)

```{r echo = F}
tips <- read_csv("data/tip-data.csv") %>%
  filter(!is.na(Party))
```

---

### Is `Meal` a significant predictor of tips?

```{r echo=F}
model.tips <- lm(Tip ~ Party + Age + Meal,data=tips)
kable(tidy(model.tips),format="html", digits=3)
```

---

### Tips data:  Nested F Test

$$\begin{aligned}&H_0: \beta_{late night} = \beta_{lunch} = 0\\
&H_a: \text{ at least one }\beta_j \text{ is not equal to 0}\end{aligned}$$

--

```{r}
reduced <- lm(Tip ~ Party + Age, data = tips)
```

--

```{r}
full <- lm(Tip ~ Party + Age + Meal, data = tips)
```

--

```{r}
kable(anova(reduced, full), format="markdown", digits = 3)
```

--

**At least one coefficient associated with `Meal` is not zero. Therefore, `Meal` is a significant predictor of `Tips`.**

---

class: middle

.question[
Why can we not rely on the individual p-values to determine if a categorical variable with $k > 2$ levels) is significant? 

*Hint*: What does it actually mean if none of the $k-1$ p-values are significant?
]

---

### Practice with Interactions 

```{r echo=F}
full <- lm(Tip ~ Party + Age + Meal + Meal*Party, data = tips)
kable(tidy(full),format="html")
```

.question[
1. What is the baseline level for `Meal`?
2. How do we expect the mean tips to change when `Meal == "Late Night"`, holding Age and Party constant? 
4. How does the slope of `Party` change when `Meal == "Late Night"`, holding Age and Party constant?
]
---

### Nested F test for interactions

**Are there any significant interaction effects with Party in the model?**

```{r}
reduced <- lm(Tip ~ Party + Age + Meal, data = tips)
```

--

```{r}
full <- lm(Tip ~ Party + Age + Meal + Age* Party + Meal * Party, 
           data = tips)
```

--

```{r}
kable(anova(reduced, full ), format="markdown", digits = 3)
```

---

### Final model for now

We conclude that there are no significant interactions with `Party` in the model. Therefore, we will use the original model that only included main effects. 


```{r echo  =F}
model.tips <- lm(Tip ~ Party + Age + Meal,data=tips)
kable(tidy(model.tips),format="html", digits=3)
```

---

class: middle, center

## Model Diagnostics

---

class: middle, center

## Influential and Leverage Points

---

### Influential Observations

An observation is <font class="vocab3">influential</font> if removing it substantially changes the coefficients of the regression model 

```{r,echo=F}
library(cowplot)
set.seed(12)
n <- 20
x <- c(runif(n,0,1))
y <- 3*x + rnorm(n,0,.5)
new.pt <- data.frame(x=2,y=0)
x.new <- c(x,2)
y.new <- c(y,0)
data <- bind_cols(x=x.new,y=y.new)
p1<- ggplot(data=data,aes(x=x,y=y))+geom_point(alpha =0.5)  + 
              geom_point(data=new.pt,color="red",size=3,shape=17) + 
  geom_smooth(method="lm",se=F) + 
  labs(title = "With Influential Point")+ theme_light()+
  theme(title=element_text(hjust=0.5,size=14)) + 
  scale_x_continuous(limits = c(0,2)) 

data2 <- bind_cols(x=x,y=y)
p2 <- ggplot(data=data2,aes(x=x,y=y))+geom_point(alpha=0.5) + geom_smooth(method="lm",se=F) + 
  labs(title="Without Influential Point") + 
  scale_x_continuous(limits = c(0, 2)) + theme_light() + theme(title=element_text(hjust=0.5,size=14))  
plot_grid(p1,p2,ncol=2)
```

---

### Influential Observations 

- In addition to the coefficients, influential observations can have a large impact on the standard errors

- Occasionally these observations can be identified in the scatterplot
  + This is often not the case - especially when dealing with multivariate data

- We will use measures to quantify an individual observation's influence on the regression model 
  + **leverage**, **standardized residuals**, and **Cook's distance**

---

### Leverage

- <font class="vocab3">Leverage: </font> measure of the distance between an observation's values of the predictor variables and the average values of the predictor variables for the whole data set
  
- An observation has high leverage if its combination of values for the predictor variables is very far from the typical combinations in the data 
  + It is **<u>potentially</u>** an influential point, i.e. may have a large impact on the coefficient estimates and standard errors


- **Note:** Identifying points with high leverage has nothing to do with the values of the response variables

---

### Calculating Leverage

- <font class="vocab">Simple Regression:</font> leverage of the $i^{th}$ observation is 
.alert[
$$h_i =  \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{j=1}^{n}(x_j-\bar{x})^2}$$
]


- <font class="vocab">Multiple Regression:</font> leverage of the $i^{th}$ observation is the $i^{th}$ diagonal of
$$\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$$

---

### High Leverage

- Values of leverage are between $\frac{1}{n}$ and 1 for each observation 

- The average leverage for all observations in the data set is $\frac{(p+1)}{n}$

- There are different thresholds for determining when an observation has **high leverage**
  + We will use the threshold  $h_i > \frac{2(p+1)}{n}$

- Observations with high leverage tend to have small residuals

---

### High Leverage

- Questions to check if you identify points with high leverage: 
  + Are they a result of data entry errors?
  + Are they in the scope for the individuals for which you want to make predictions?
  + Are they impacting the estimates of the model coefficients, especially for interactions?

- Just because a point has high leverage does not necessarily mean it will have a substantial impact on the regression. Therefore you should check other measures.

---

### Standardized & Studentized Residuals

- What is the best way to identify outliers (points that don't fit the pattern from the regression line)? 
  
--

- Look for points that have large residuals

--

- We want a common scale, so we can more easily identify "large" residuals

--

- We will look at each residual divided by its standard error

---

### Standardized Residuals 

.alert[
$$std.res_i = \frac{e_i}{\hat{\sigma}\sqrt{1-h_i}}$$
]
<br>

--

- The standard error of a residual, $\hat{\sigma}\sqrt{1-h_i}$ depends on the value of the predictor variables 

--


- Residuals for observations that are high leverage have smaller variance than residuals for observations that are low leverage 

  + This is because the regression line tries to fit high leverage observations as closely as possible
  
---

### Standardized Residuals 

- Values with very large standardized residuals are outliers, since they don't fit the pattern determined by the regression model 

- Observations with standardized residuals of magnitude $> 2$ should be examined more closely

- Observations with large standardized residuals are outliers but may not have an impact on the regression line

- <font class="vocab">Good Practice: </font>Make residual plots with standardized residuals
    - It is easier to identify outliers and check for constant variance assumption

---

### Motivating Cook's Distance

- If a observation has a large impact on the estimated regression coefficients, when we drop that observation...
  + The estimated coefficients should change 
  + The predicted $Y$ value for that observation should change
  
- One way to determine each observation's impact could be to delete it, rerun the regression, compare the predicted $Y$ values from the new and original models
  + This could be very time consuming 
  
- Instead, we can use <font class="vocab3">Cook's Distance</font> which gives a measure of the change in the predicted $Y$ value when an observation is dropped

---

### Cook's Distance

- <font class="vocab3">Cook's Distance: </font> Measure of an observation's overall impact, i.e. the effect removing the observation has on the estimated coefficients

- For the $i^{th}$ observation, we can calculate Cook's Distance as 
.alert[
$$D_i = \frac{1}{p}(std.res_i)^2\bigg(\frac{h_i}{1-h_i}\bigg)$$
]

- *Note:* Cook's distance, $D_i$, incorporates both the residual and the leverage for each observation

- An observation with large $D_i$ is said to have a strong influence on the predicted values 

---

### Using these measures

- Standardized residuals, leverage, and Cook's Distance should all be examined together 

- Examine plots of the measures to identify observations that may have an impact on your regression model 

- Some thresholds for flagging potentially influential observations:
  + **Leverage**: $h_i > \frac{2(p+1)}{n}$ (some software uses $2p/n$)
  + **Standardized Residuals**: $|std.res_i| > 2$
  + **Cook's Distance**: $D_i > 1$

---

### What to do with outliers/influential observations?

- It is **<font color="green">OK</font>** to drop an observation based on the **<u>predictor variables</u>** if...
    - It is meaningful to drop the observation given the context of the problem 
    - You intended to build a model on a smaller range of the predictor variables. Mention this in the write up of the results and be careful to avoid extrapolation when making predictions

--

- It is **<font color="red">not OK</font>** to drop an observation based on the response variable
    - These are legitimate observations and should be in the model
- You can try transformations or increasing the sample size by collecting more data

--

- In either instance, you can try building the model with and without the outliers/influential observations

---

### Model diagnostics in R

- Use the <font class="vocab">`augment`</font> function in the broom package to output the model diagnostics (along with the predicted values and residuals)

- Output from `augment` : 
    - response and predictor variables in the model
    - `.fitted`: predicted values
    - `.se.fit`: standard errors of predicted values
    - `.resid`: residuals
    - **`.hat`**: leverage
    - `.sigma`: estimate of residual standard deviation when corresponding observation is dropped from model
    - **`.cooksd`**: Cook's distance
    - **`.std.resid`**: standardized residuals

---

### Example: Restaurant tips

What affects the amount customers tip at a restaurant?

- **Response:**
    - <font class="vocab">`Tip`</font>: amount of the tip
    
- **Predictors:**
    - <font class="vocab">`Party`</font>: number of people in the party
    - <font class="vocab">`Meal`</font>:  time of day (Lunch, Dinner, Late Night) 
    - <font class="vocab">`Age`</font>: age category of person paying the bill (Yadult, Middle, SenCit)

```{r}
tips <- read_csv("data/tip-data.csv") %>%
  filter(!is.na(Party))
```


---

### Example: Tips

```{r}
model1 <- lm(Tip ~ Party + Meal + Age , data = tips)
kable(tidy(model1),format="html",digits=3)
```

---

### Using `augment` function

- Use the `augment` function to add predicted values and model diagnostics to data
    - Add the observation number for diagnostic plots
    
```{r}
tips_output <- augment(model1) %>%
  mutate(obs_num = row_number())
```

---

### Augmented data

```{r}
glimpse(tips_output)
```

---

### Leverage

```{r}
leverage_threshold <- 2*(5+1)/nrow(tips)
```

```{r}
ggplot(data = tips_output, aes(x = obs_num,y = .hat)) + 
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept = leverage_threshold,color = "red")+
  labs(x = "Observation Number",y = "Leverage",title = "Leverage") +
  geom_text(aes(label=ifelse(.hat > leverage_threshold, as.character(obs_num), "")), nudge_x = 4)
```

---

### Points with high leverage

```{r}
tips_output %>% filter(.hat > leverage_threshold) %>% 
  select(Party, Meal, Age)
```

.question[
Why do you think these points have high leverage?
]

---

### Standardized Residuals vs. Predicted

```{r}
ggplot(data = tips_output, aes(x = .fitted,y = .std.resid)) +
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept = 0,color = "red") +
  geom_hline(yintercept = -2,color = "red",linetype = "dotted") +
  geom_hline(yintercept = 2,color = "red",linetype = "dotted") +
  labs(x ="Predicted Value",y ="Standardized Residuals",title = "Standardized Residuals vs. Predicted") +
  geom_text(aes(label = ifelse(abs(.std.resid) >2,as.character(obs_num),"")), nudge_x = 0.3)
```

---

### Standardized residuals vs. predictors 

```{r echo=F, fig.height = 5}
p1 <- ggplot(data = tips_output, aes(x = Party, y=.std.resid)) + 
  geom_point() + 
  geom_hline(yintercept = 0,color = "red")
p2 <- ggplot(data=tips_output, aes(x = Meal, y = .std.resid)) + 
  geom_boxplot() + 
  geom_hline(yintercept=0,color="red")
p3 <- ggplot(data = tips_output, aes(x = Age, y = .std.resid)) + 
  geom_boxplot() + 
  geom_hline(yintercept=0,color="red")
plot_grid(p1,p2,p3,ncol=2)
```

---

### Points with large magnitude std.res.

```{r}
tips_output %>% filter(abs(.std.resid) > 2) %>% 
  select(Party, Meal, Age, Tip)
```

.question[
- Why do you think these points have standardized residuals with large magnitude?
- What other variables could you examine?
]

---

### Why we want to find outliers

Estimate of regression standard deviation, $\hat{\sigma}$, using all observations

```{r}
glance(model1)$sigma
```

Estimate of $\hat{\sigma}$ without points with large magnitude standardized residuals

```{r}
tips_output %>%
  filter(abs(.std.resid) <= 2) %>%
  summarise(sigma_est = sqrt(sum(.resid^2)/(n() - 5 - 1)))
```

---

class: middle 

**Recall that we use $\hat{\sigma}$ to calculate the standard errors for all confidence intervals and p-values, so outliers can affect conclusions drawn from model**

---

### Cook's Distance

```{r}
ggplot(data = tips_output, aes(x = obs_num, y = .cooksd)) + 
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept=1,color = "red")+
  labs(x= "Observation Number",y = "Cook's Distance",title = "Cook's Distance") +
  geom_text(aes(label = ifelse(.hat>1,as.character(obs_num),"")))
```

---

class: middle

See the supplemental notes [Details on Model Diagnostics](https://github.com/sta210-fa19/supplemental-notes/blob/master/model-diagnostics-matrix.pdf) for more details about standardized residuals, leverage points, and Cook's distance.


---

class: middle, center

## Multicollinearity

---

### Why multicollinearity is a problem

- We can't include two variables that have a perfect linear association with each other

- If we did so, we could not pick a unique best fit model


---

### Why multicollinearity is a problem

- Ex. Suppose the true population regression equation is $y = 3 + 4x$

--

- Suppose we try estimating that regression model using the variables $x$ and $z = x/10$
$$\begin{aligned}\hat{y}&= \hat{\beta}_0 + \hat{\beta}_1x  + \hat{\beta}_2\frac{x}{10}\\
&= \hat{\beta}_0 + \bigg(\hat{\beta}_1 + \frac{\hat{\beta}_2}{10}\bigg)x\end{aligned}$$

--

- We can set $\hat{\beta}_1$ and $\hat{\beta}_2$ to any two numbers such that $\hat{\beta}_1 + \frac{\hat{\beta}_2}{10} = 4$
  + We are unable then to choose the "best" combination of $\hat{\beta}_1$ and $\hat{\beta}_2$
  
---

### Why multicollinearity is a problem

- When we have almost perfect collinearities (i.e. highly correlated explanatory variables), the standard errors for our regression coefficients inflate

- In other words, we lose precision in our estimates of the regression coefficients 
  
---
 
### Detecting Multicollinearity

Multicollinearity may occur when...
- There are very high correlations $(r > 0.9)$ among two or more explanatory variables, especially for smaller sample sizes

--

- One (or more) explanatory variables is an almost perfect linear combination of the others 

--

- Include quadratic terms without first mean-centering the variables before squaring

--

- Including interactions with two or more continuous variables

---

 

### Detecting Multicollinearity 

- Look at a correlation matrix of the predictor variables, including all indicator variables 
  + Look out for values close to 1 or -1

- If you think one predictor variable is an almost perfect linear combination of other predictor variables, you can run a regression of that predictor variable vs. the others and see if $R^2$ is close to 1

---

### Detecting Multicollinearity (VIF)

- <font class="vocab">Variance Inflation Factor (VIF)</font>: Measure of multicollinearity 

.alert[
$$VIF(\hat{\beta}_j) = \frac{1}{1-R^2_{X_j|X_{-j}}}$$
]

where $R^2_{X_j|X_{-j}}$ is the proportion of variation $X$ that is explained by the linear combination of the other explanatory variables in the model.


- Typically $VIF > 10$ indicates concerning multicollinearity


- Use the <font class="vocab">`vif()`</font> function in the `rms` package to calculate VIF

---

### Tips VIF

- Calculate VIF using the <font class="vocab">`vif`</font> function in the rms package 

```{r}
library(rms)
tidy(vif(model1))
```

---

### Calculating VIF for `Party`

```{r}
party_model <- lm(Party ~ Meal + Age, data=tips)
r.sq <- glance(party_model)$r.squared
(vif <- 1/(1-r.sq))
```

---

### Calculating VIF for `MealLateNight`

```{r}
# create indicator variables for Meal
tips <- tips %>% 
  mutate(late_night = if_else(Meal=="Late Night",1,0), 
         lunch = if_else(Meal=="Lunch",1,0))
```

```{r}
late_night_model <- lm(late_night ~ lunch + Party + Age, data=tips)
r.sq <- glance(late_night_model)$r.squared
(vif <- 1/(1-r.sq))
```





